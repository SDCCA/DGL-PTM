{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/victoria/Documents/Scripts/Python/DGL-PTM/dgl_ptm')\n",
    "import dgl_ptm\n",
    "import os\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dgl_ptm.PovertyTrapModel(model_identifier='testwithnn',restart=False, savestate=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "{'edata': ['all'], 'epath': 'testwithnn/edge_data', 'format': 'xarray', 'mode': 'w', 'ndata': ['all_except', ['a_table']], 'npath': 'testwithnn/agent_data.zarr', 'wealth_method': 'weighted_transfer', 'income_method': 'income_generation', 'consume_method': 'past_shock_bellman_consumption', 'nn_path': '/Users/victoria/Documents/Scripts/Python/DGL-PTM/DGL_testing/nn_data/both_PudgeSixLayer_1024/0723_110813/model_best.pth', 'capital_update_method': 'default', 'characteristic_distance': 35, 'homophily_parameter': 0.69, 'adapt_m': tensor([0.0000, 0.5000, 0.9000]), 'adapt_cost': tensor([0.0000, 0.2500, 0.4500]), 'depreciation': 0.6, 'discount': 0.95, 'm_theta_dist': {'type': 'multinomial', 'parameters': [tensor([0.0200, 0.0300, 0.0500, 0.9000]), tensor([0.7000, 0.8000, 0.9000, 1.0000])], 'round': False, 'decimals': None}, 'tech_gamma': tensor([0.3000, 0.3500, 0.4500]), 'tech_cost': tensor([0.0000, 0.1500, 0.6500]), 'del_method': 'size', 'del_threshold': 0.05, 'noise_ratio': 0.05, 'local_ratio': 0.25, 'truncation_weight': 1e-10, 'step_type': 'custom'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model.set_model_parameters(**{'number_agents': 100 , \n",
    "    'seed':0,\n",
    "    'gamma_vals':torch.tensor([0.3,0.45]) , #for pseudo income\n",
    "    'sigma_dist': {'type':'uniform','parameters':[0.05,1.94],'round':True,'decimals':1},\n",
    "    'cost_vals': torch.tensor([0.,0.45]), #for pseudo income\n",
    "    'a_theta_dist': {'type':'uniform','parameters':[0.1,1],'round':False,'decimals':None},\n",
    "    'sensitivity_dist':{'type':'uniform','parameters':[0.0,1],'round':False,'decimals':None},\n",
    "    'capital_dist': {'type':'uniform','parameters':[0.1,10.],'round':False,'decimals':None}, \n",
    "    'alpha_dist': {'type':'normal','parameters':[1.08,0.074],'round':False,'decimals':None},\n",
    "    'lambda_dist': {'type':'uniform','parameters':[0.05,0.94],'round':True,'decimals':1},\n",
    "    'initial_graph_type': 'barabasi-albert',\n",
    "    'initial_graph_args': {'seed': 0, 'new_node_edges':1},\n",
    "    'device': 'cpu',\n",
    "    'step_count':0,\n",
    "    'step_target':20,\n",
    "    'steering_parameters':{'npath':'./agent_data.zarr',\n",
    "                            'epath':'./edge_data', \n",
    "                            'ndata':['all_except',['a_table']],\n",
    "                            'edata':['all'],\n",
    "                            'mode':'w',\n",
    "                            'wealth_method':'weighted_transfer',\n",
    "                            'income_method':'income_generation',\n",
    "                            'tech_gamma': torch.tensor([0.3,0.35,0.45]),\n",
    "                            'tech_cost': torch.tensor([0,0.15,0.65]),\n",
    "                            'consume_method':'past_shock_bellman_consumption',\n",
    "                            'nn_path': \"/Users/victoria/Documents/Scripts/Python/DGL-PTM/DGL_testing/nn_data/both_PudgeSixLayer_1024/0723_110813/model_best.pth\",\n",
    "                            'adapt_m':torch.tensor([0,0.5,0.9]),\n",
    "                            'adapt_cost':torch.tensor([0,0.25,0.45]),\n",
    "                            'depreciation': 0.6,\n",
    "                            'discount': 0.95,\n",
    "                            'm_theta_dist': {'type':'multinomial','parameters':[[0.02 ,0.03, 0.05, 0.9],[0.7, 0.8, 0.9, 1]],'round':False,'decimals':None},\n",
    "                            'del_method':'size',\n",
    "                            'del_threshold':0.05,\n",
    "                            'ratio':0.1,\n",
    "                            'weight_a':0.69,\n",
    "                            'weight_b':35, \n",
    "                            'truncation_weight':1.0e-10,\n",
    "                            'step_type':'custom'}})\n",
    "\n",
    "print(model.number_agents)  \n",
    "print(model.steering_parameters)                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using seed 0 for network creation.\n",
      "1 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victoria/Documents/Scripts/Python/DGL-PTM/dgl_ptm/dgl_ptm/model/initialize_model.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dist = torch.gather(torch.Tensor(distParameters[1]), 0, torch.multinomial(torch.tensor(distParameters[0]), nSamples, replacement=True))\n"
     ]
    }
   ],
   "source": [
    "model.initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing step 0 of 20\n",
      "Agents in equation violation: 7\n",
      "Agents in personal/actual violation: 4\n",
      "...because of i_a: 0\n",
      "performing step 1 of 20\n",
      "Agents in equation violation: 4\n",
      "Agents in personal/actual violation: 4\n",
      "...because of i_a: 0\n",
      "performing step 2 of 20\n",
      "Agents in equation violation: 3\n",
      "Agents in personal/actual violation: 3\n",
      "...because of i_a: 0\n",
      "performing step 3 of 20\n",
      "Agents in equation violation: 4\n",
      "Agents in personal/actual violation: 4\n",
      "...because of i_a: 0\n",
      "performing step 4 of 20\n",
      "Agents in equation violation: 8\n",
      "Agents in personal/actual violation: 8\n",
      "...because of i_a: 0\n",
      "performing step 5 of 20\n",
      "Agents in equation violation: 15\n",
      "Agents in personal/actual violation: 15\n",
      "...because of i_a: 0\n",
      "performing step 6 of 20\n",
      "Agents in equation violation: 17\n",
      "Agents in personal/actual violation: 17\n",
      "...because of i_a: 0\n",
      "performing step 7 of 20\n",
      "Agents in equation violation: 19\n",
      "Agents in personal/actual violation: 19\n",
      "...because of i_a: 0\n",
      "performing step 8 of 20\n",
      "Agents in equation violation: 20\n",
      "Agents in personal/actual violation: 20\n",
      "...because of i_a: 0\n",
      "performing step 9 of 20\n",
      "Agents in equation violation: 22\n",
      "Agents in personal/actual violation: 22\n",
      "...because of i_a: 0\n",
      "performing step 10 of 20\n",
      "Agents in equation violation: 22\n",
      "Agents in personal/actual violation: 22\n",
      "...because of i_a: 0\n",
      "performing step 11 of 20\n",
      "Agents in equation violation: 22\n",
      "Agents in personal/actual violation: 22\n",
      "...because of i_a: 0\n",
      "performing step 12 of 20\n",
      "Agents in equation violation: 24\n",
      "Agents in personal/actual violation: 24\n",
      "...because of i_a: 0\n",
      "performing step 13 of 20\n",
      "Agents in equation violation: 24\n",
      "Agents in personal/actual violation: 24\n",
      "...because of i_a: 0\n",
      "performing step 14 of 20\n",
      "Agents in equation violation: 26\n",
      "Agents in personal/actual violation: 26\n",
      "...because of i_a: 0\n",
      "performing step 15 of 20\n",
      "Agents in equation violation: 26\n",
      "Agents in personal/actual violation: 26\n",
      "...because of i_a: 0\n",
      "performing step 16 of 20\n",
      "Agents in equation violation: 28\n",
      "Agents in personal/actual violation: 28\n",
      "...because of i_a: 0\n",
      "performing step 17 of 20\n",
      "Agents in equation violation: 27\n",
      "Agents in personal/actual violation: 27\n",
      "...because of i_a: 0\n",
      "performing step 18 of 20\n",
      "Agents in equation violation: 15\n",
      "Agents in personal/actual violation: 16\n",
      "...because of i_a: 0\n",
      "performing step 19 of 20\n",
      "Agents in equation violation: 19\n",
      "Agents in personal/actual violation: 19\n",
      "...because of i_a: 0\n"
     ]
    }
   ],
   "source": [
    "model.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.9567, 2.9776, 2.1575, 3.4204, 2.7872])\n",
      "tensor([1, 2, 2, 2, 2])\n",
      "torch.float32\n",
      "tensor([1, 2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "TechTable =  np.array([[0.3,0],[0.35,0.15],[0.45, 0.65]])\n",
    "wealth=torch.tensor([ 6.4072, 17.6514,  8.5695, 27.9308, 13.7166])\n",
    "alpha=torch.tensor([1.0997, 0.9967, 1.0678, 0.9097, 1.0579])\n",
    "gamma = torch.tensor([0.3,0.35,0.45])\n",
    "cost = torch.tensor([0,0.15,0.65])\n",
    "\n",
    "income,index = torch.max((alpha[:,None]*wealth[:,None]**TechTable[:,0] - TechTable[:,1]), axis=1)\n",
    "income=income.to(torch.float32)\n",
    "\n",
    "print(income)\n",
    "print(index)\n",
    "\n",
    "\n",
    "income,tech_index = torch.max((alpha[:,None]*wealth[:,None]**gamma - cost), axis=1)\n",
    "\n",
    "print(income.dtype)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.8976, 1.2028, 1.0325, 6.1137, 8.8345, 6.9923, 3.7123, 2.1701, 3.4315,\n",
      "        1.9355])\n",
      "tensor(0.8000)\n"
     ]
    }
   ],
   "source": [
    "def sample_distribution_tensor(type, distParameters, nSamples, round=False, decimals=None):\n",
    "    \"\"\"\n",
    "    create and return samples from different distributions\n",
    "\n",
    "    :param type: Type of distribution to sample\n",
    "    :param distParameters: array of parameters as required/supported by requested distribution type\n",
    "    :param nSamples: number of samples to return (as 1d tensor)\n",
    "    :param round: optional, whether the samples are to be rounded\n",
    "    :param decimals: optional, required if round is specified. decimal places to round to\n",
    "    \"\"\"\n",
    "    if type == 'uniform':\n",
    "        dist = torch.distributions.uniform.Uniform(torch.tensor(distParameters[0]),torch.tensor(distParameters[1])).sample(torch.tensor([nSamples]))\n",
    "    elif type == 'normal':\n",
    "        dist = torch.distributions.normal.Normal(torch.tensor(distParameters[0]),torch.tensor(distParameters[1])).sample(torch.tensor([nSamples]))\n",
    "    elif type == 'bernoulli':\n",
    "        dist = torch.distributions.bernoulli.Bernoulli(probs=distParameters[0],logits=distParameters[1],validate_args=None).sample(torch.tensor([nSamples]))\n",
    "    elif type == 'multinomial':\n",
    "        dist = torch.gather(torch.Tensor(distParameters[1]), 0, torch.multinomial(torch.tensor(distParameters[0]), nSamples, replacement=True))\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError('Currently only uniform, normal, multinomial, and bernoulli distributions are supported')\n",
    "\n",
    "    if round:\n",
    "        if decimals == None:\n",
    "            raise ValueError('rounding requires decimals of rounding accuracy to be specified')\n",
    "        else:\n",
    "            return torch.round(dist,decimals=decimals)\n",
    "    else:\n",
    "        return dist\n",
    "\n",
    "theta_dist = {'type':'multinomial','parameters':[[0.01 ,0.1, 0.79, 0.1],[0.1, 0.5, 0.8, 1]],'round':False,'decimals':None} \n",
    "theta_vals = torch.tensor([0.1, 0.5, 0.8, 1])\n",
    "capital_dist = {'type':'uniform','parameters':[0.1,10.],'round':False,'decimals':None}\n",
    "\n",
    "\n",
    "agentsCapital = sample_distribution_tensor(capital_dist['type'],capital_dist['parameters'],10,capital_dist['round'],decimals=capital_dist['decimals'])\n",
    "print(agentsCapital)\n",
    "modelTheta = sample_distribution_tensor(theta_dist['type'],theta_dist['parameters'],10,round=theta_dist['round'],decimals=theta_dist['decimals'])\n",
    "print(modelTheta[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(1.2311), tensor(1.1246), tensor(0.7160)]\n",
      "tensor([1.2311, 1.1246, 0.7160])\n"
     ]
    }
   ],
   "source": [
    "tech_gamma=torch.tensor([0.3,0.35,0.45])\n",
    "tech_cost=torch.tensor([0,0.15,0.65])\n",
    "α=1\n",
    "k=2\n",
    "\n",
    "\n",
    "\n",
    "f = []\n",
    "for i in range(tech_gamma.size(dim=0)): \n",
    "    #in the end, they may need their own tech tables\n",
    "    entry = α * k**tech_gamma[i] - tech_cost[i]\n",
    "    f.append(entry)\n",
    "print(f)\n",
    "entry = α * k**tech_gamma - tech_cost\n",
    "print(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3000, 0.3500, 0.4500],\n",
       "         [0.0000, 0.1500, 0.6500]],\n",
       "\n",
       "        [[0.3000, 0.3500, 0.4500],\n",
       "         [0.0000, 0.1500, 0.6500]],\n",
       "\n",
       "        [[0.3000, 0.3500, 0.4500],\n",
       "         [0.0000, 0.1500, 0.6500]],\n",
       "\n",
       "        [[0.3000, 0.3500, 0.4500],\n",
       "         [0.0000, 0.1500, 0.6500]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([tech_gamma,tech_cost]).repeat(4,1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1, 0.5, 0.8, 1]\n",
      "['a', 'b', 'c']\n",
      "['b', 'c']\n",
      "['b']\n"
     ]
    }
   ],
   "source": [
    "data={'a':0,'b':4,'c':12}\n",
    "params={'steering_parameters':{'m_theta_dist': {'type':'multinomial','parameters':[[0.01 ,0.1, 0.79, 0.1],[0.1, 0.5, 0.8, 1]],'round':False,'decimals':None},\n",
    "                            'del_prob':0.05},'ndtata':['all']}\n",
    "\n",
    "print(params['steering_parameters']['m_theta_dist']['parameters'][1])\n",
    "\n",
    "\n",
    "def printdata(ndata):\n",
    "    if ndata == ['all']:\n",
    "        ndata = list(data.keys())\n",
    "\n",
    "    if ndata[0] == 'allexcept':\n",
    "        ndata = list(data.keys() - ndata[1])\n",
    "    print(ndata)\n",
    "\n",
    "\n",
    "printdata(['all'])\n",
    "printdata(['allexcept',['a']])\n",
    "printdata(['allexcept',['a','c']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'theta': tensor([0.7603, 0.7641, 0.9275, 0.7906, 0.7858, 0.7832, 0.3904, 0.3924, 0.7884]), 'sigma': tensor([1.3000, 1.8000, 0.6000, 0.7000, 1.2000, 1.1000, 0.7000, 1.8000, 1.4000]), 'alpha': tensor([0.9952, 1.1045, 1.0706, 1.0489, 1.0576, 1.1922, 1.0162, 1.0938, 1.1324]), 'wealth': tensor([0.9998, 1.1309, 1.1439, 1.0342, 1.2321, 1.0819, 1.0326, 1.1193, 1.0707]), 'a_table': tensor([[[0.0000, 0.5000, 0.9000],\n",
      "         [0.0000, 0.2500, 0.4500]],\n",
      "\n",
      "        [[0.0000, 0.5000, 0.9000],\n",
      "         [0.0000, 0.2500, 0.4500]],\n",
      "\n",
      "        [[0.0000, 0.5000, 0.9000],\n",
      "         [0.0000, 0.2500, 0.4500]],\n",
      "\n",
      "        [[0.0000, 0.5000, 0.9000],\n",
      "         [0.0000, 0.2500, 0.4500]],\n",
      "\n",
      "        [[0.0000, 0.5000, 0.9000],\n",
      "         [0.0000, 0.2500, 0.4500]],\n",
      "\n",
      "        [[0.0000, 0.5000, 0.9000],\n",
      "         [0.0000, 0.2500, 0.4500]],\n",
      "\n",
      "        [[0.0000, 0.5000, 0.9000],\n",
      "         [0.0000, 0.2500, 0.4500]],\n",
      "\n",
      "        [[0.0000, 0.5000, 0.9000],\n",
      "         [0.0000, 0.2500, 0.4500]],\n",
      "\n",
      "        [[0.0000, 0.5000, 0.9000],\n",
      "         [0.0000, 0.2500, 0.4500]]]), 'wealth_consumption': tensor([4.4039e-01, 5.6813e-01, 3.5487e-01, 3.7399e-01, 5.1065e-01, 5.0570e-01,\n",
      "        3.3307e-01, 3.5862e-04, 5.3021e-01]), 'i_a': tensor([0.4500, 0.4500, 0.4500, 0.4500, 0.4500, 0.4500, 0.0000, 0.0000, 0.4500]), 'm': tensor([0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.0000, 0.0000, 0.9000])}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "def income_function(k,α,tech): \n",
    "    f=α * k**tech['gamma'] - tech['cost']\n",
    "    return torch.max(f)\n",
    "\n",
    "\n",
    "class BellmanEquation:\n",
    "    #Adapted from: https://python.quantecon.org/optgrowth.html\n",
    "    def __init__(self,\n",
    "                u,            # utility function\n",
    "                f,            # production function\n",
    "                k,            # current state k_t\n",
    "                θ,            # given shock factor θ\n",
    "                σ,            # risk averseness\n",
    "                α,            # human capital\n",
    "                i_a,          # adaptation investment\n",
    "                m,            # protection multiplier\n",
    "                β,            # discount factor\n",
    "                𝛿,            # depreciation factor \n",
    "                tech):       # adaptation table \n",
    "                #name=\"BellmanNarrowExtended\"\n",
    "                \n",
    "\n",
    "        self.u, self.f, self.k, self.β, self.θ, self.𝛿, self.σ, self.α, self.i_a, self.m, self.tech = u, f, k, β, θ, 𝛿, σ, α, i_a, m, tech\n",
    "\n",
    "        # Set up grid\n",
    "        \n",
    "        startgrid=np.array([1.0e-7,1,2,3,4,5,6,7,8,9,10,k+100])\n",
    "\n",
    "        ind=np.searchsorted(startgrid, k)\n",
    "        self.grid=np.concatenate((startgrid[:ind],np.array([k*0.99999, k]),\n",
    "                                startgrid[ind:]))\n",
    "\n",
    "        self.grid=self.grid[self.grid>i_a]\n",
    "\n",
    "        # Identify target state k\n",
    "        self.index = np.searchsorted(self.grid, k)-1\n",
    "    def value(self, c, y, v_array):\n",
    "        \"\"\"\n",
    "        Right hand side of the Bellman equation.\n",
    "        \"\"\"\n",
    "\n",
    "        u, f, β, θ, 𝛿, σ, α, i_a, m, tech = self.u, self.f, self.β, self.θ, self.𝛿, self.σ, self.α, self.i_a, self.m, self.tech\n",
    "\n",
    "        v = interp1d(self.grid, v_array, bounds_error=False, \n",
    "                    fill_value=\"extrapolate\")\n",
    "        return u(c,σ) + β * v((θ + m * (1-θ)) * (f(y,α,tech) - c - i_a + (1 - 𝛿) * y))\n",
    "\n",
    "\n",
    "def _optimized_wealth_consumption(model_graph, model_params):\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "    def maximize(g, a, b, args):\n",
    "        \"\"\"\n",
    "        From: https://python.quantecon.org/optgrowth.html (similar example \n",
    "        https://macroeconomics.github.io/Dynamic%20Programming.html#.ZC13-exBy3I)\n",
    "        Maximize the function g over the interval [a, b].\n",
    "\n",
    "        The maximizer of g on any interval is\n",
    "        also the minimizer of -g.  The tuple args collects any extra\n",
    "        arguments to g.\n",
    "\n",
    "        Returns the maximum value and the maximizer.\n",
    "        \"\"\"\n",
    "\n",
    "        objective = lambda x: -g(x, *args)\n",
    "        result = minimize_scalar(objective, bounds=(a, b), method='bounded')\n",
    "        maximizer, maximum = result.x, -result.fun\n",
    "        return maximizer, maximum\n",
    "\n",
    "    def utility(c, σ, type=\"isoelastic\"):\n",
    "        if type == \"isoelastic\":\n",
    "            if σ ==1:\n",
    "                return np.log(c)\n",
    "            else:\n",
    "                return (c**(1-σ)-1)/(1-σ)\n",
    "\n",
    "        else:\n",
    "            print(\"Unspecified utility function!!!\")\n",
    "\n",
    "\n",
    "    def update_bellman(v, bell):\n",
    "        \"\"\"\n",
    "        From: https://python.quantecon.org/optgrowth.html (similar example\n",
    "        https://macroeconomics.github.io/Dynamic%20Programming.html#.ZC13-exBy3I)\n",
    "        \n",
    "        The Bellman operator.  Updates the guess of the value function\n",
    "        and also computes a v-greedy policy.\n",
    "\n",
    "        * bell is an instance of Bellman equation\n",
    "        * v is an array representing a guess of the value function\n",
    "\n",
    "        \"\"\"\n",
    "        v_new = np.empty_like(v)\n",
    "        v_greedy = np.empty_like(v)\n",
    "        \n",
    "        for i in range(len(bell.grid)):\n",
    "            y = bell.grid[i]\n",
    "            # Maximize RHS of Bellman equation at state y\n",
    "            \n",
    "            c_star, v_max = maximize(bell.value, min([1e-8,y*0.00001]), \n",
    "                                    y-bell.i_a, (y, v))\n",
    "            #VMG HELP! can anyone check that (1) subtracting i_a and \n",
    "            # (2) omitting any grid values less than i_a \n",
    "            # will not be problematic? The only thing I can come up with\n",
    "            # is if i_a is greater than k*0.99999\n",
    "            # which_bellman() now accounts for that case. Whole thing \n",
    "            # could use refinement.\n",
    "        \n",
    "            v_new[i] = v_max\n",
    "            v_greedy[i] = c_star\n",
    "\n",
    "        return v_greedy, v_new\n",
    "\n",
    "\n",
    "    def which_bellman(agentinfo):\n",
    "        \"\"\"\n",
    "        Solves bellman for each affordable adaptation option.\n",
    "        \"\"\"\n",
    "        feasible=[]\n",
    "\n",
    "        for option in torch.transpose(agentinfo['adapt'],0,1):\n",
    "            if option[1]>=((income_function(agentinfo['k'],agentinfo['α'],tech={'gamma':model_params['tech_gamma'],'cost':model_params['tech_cost']})+(1 - model_params['depreciation'])*agentinfo['k'])*.99998):\n",
    "                # ensures that the gridpoint\n",
    "                # just below k, k*0.99999, is included\n",
    "                pass\n",
    "            else:\n",
    "                #  print(f'working theta = {agentinfo.θ + option[0] *\\\n",
    "                #  (1-agentinfo.θ)}, i_a= {option[1]}, k= {agentinfo.k}')\n",
    "                c,v=solve_bellman(BellmanEquation(u=utility, \n",
    "                                f=income_function, k=agentinfo['k'], \n",
    "                                θ=agentinfo['θ'], σ=agentinfo['σ'], \n",
    "                                α=agentinfo['α'], i_a=option[1].numpy(),m=option[0],\n",
    "                                β=model_params['discount'], 𝛿=model_params['depreciation'],\n",
    "                                tech={'gamma':model_params['tech_gamma'],'cost':model_params['tech_cost']}))\n",
    "                feasible.append([v,c,option[1],option[0]])\n",
    "\n",
    "        best=min(feasible)\n",
    "\n",
    "        return best[1],best[2],best[3]\n",
    "\n",
    "    def solve_bellman(bell,\n",
    "                    tol=1,\n",
    "                    min_iter=10,\n",
    "                    max_iter=1000,\n",
    "                    verbose=False):\n",
    "        \"\"\"\n",
    "        From: https://python.quantecon.org/optgrowth.html (similar example\n",
    "        https://macroeconomics.github.io/Dynamic%20Programming.html#.ZC13-exBy3I)\n",
    "        \n",
    "        Solve model by iterating with the Bellman operator.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # Set up loop\n",
    "\n",
    "        v = bell.u(bell.grid,bell.σ)  # Initial condition\n",
    "        i = 0\n",
    "        error = tol + 1\n",
    "\n",
    "        while (i < max_iter and error > tol) or (i < min_iter):\n",
    "            v_greedy, v_new = update_bellman(v, bell)\n",
    "            error = np.abs(v[bell.index] - v_new)[bell.index]\n",
    "            i += 1\n",
    "            # if verbose and i % print_skip == 0:\n",
    "            #     print(f\"Error at iteration {i} is {error}.\")\n",
    "            v = v_new\n",
    "\n",
    "        if error > tol:\n",
    "            print(f\"{bell.name} failed to converge for k={bell.k}, α = {bell.α},σ ={bell.σ}, i_a={bell.i_a}, and modified θ = {bell.θ + bell.m * (1-bell.θ)}!\")\n",
    "        elif verbose:\n",
    "            print(f\"Converged in {i} iterations.\")\n",
    "            print(f\"Effective k and new c {np.around(bell.grid[bell.index],3),v_greedy[bell.index]}\")\n",
    "            \n",
    "\n",
    "        return v_greedy[bell.index],v[bell.index]\n",
    "    \n",
    "    model_graph['wealth_consumption'], model_graph['i_a'], model_graph['m'] = torch.zeros(9),torch.zeros(9),torch.zeros(9)\n",
    "    for i in range(model_params['number_agents']):\n",
    "        agentinfo = {'u':utility, 'f':income_function, 'β':model_params['discount'], 'θ':model_graph['theta'][i], '𝛿':model_params['depreciation'], 'σ':model_graph['sigma'][i].numpy(), 'α': model_graph['alpha'][i],'k':model_graph['wealth'][i],'adapt': model_graph['a_table'][i]}\n",
    "        model_graph['wealth_consumption'][i], model_graph['i_a'][i], model_graph['m'][i] = which_bellman(agentinfo)\n",
    "    print(model_graph)\n",
    "\n",
    "modelpar={'discount':0.95,'depreciation':0.08,'number_agents':9,'tech_gamma': torch.tensor([0.3,0.35,0.45]),'tech_cost': torch.tensor([0,0.15,0.65]),'num_nodes':9}        \n",
    "ndata={'theta':torch.tensor([0.7603, 0.7641, 0.9275, 0.7906, 0.7858, 0.7832, 0.3904, 0.3924, 0.7884]),'sigma':torch.tensor([1.3000, 1.8000, 0.6000, 0.7000, 1.2000, 1.1000, 0.7000, 1.8000, 1.4000]),'alpha':torch.tensor([0.9952, 1.1045, 1.0706, 1.0489, 1.0576, 1.1922, 1.0162, 1.0938, 1.1324]),'wealth':torch.tensor([0.9998, 1.1309, 1.1439, 1.0342, 1.2321, 1.0819, 1.0326, 1.1193, 1.0707]), 'a_table':torch.stack([torch.tensor([0,0.5,0.9]),torch.tensor([0,0.25,0.45])]).repeat(modelpar['number_agents'],1,1)}\n",
    "_optimized_wealth_consumption(ndata,modelpar)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "0.3666666775941849 533.3333016633987\n",
      "tensor([3.6667e-01, 5.3333e+02])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class torchInterp1d:\n",
    "    \"\"\"\n",
    "    PyTorch-friendly, 1-D linear interpolation.\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "       \n",
    "\n",
    "    def __call__(self, x_target):\n",
    "        x = self.x\n",
    "        y = self.y\n",
    "\n",
    "        y_target = torch.empty_like(x_target)\n",
    "\n",
    "        for i, x_val in enumerate(x_target):\n",
    "            if x_val < x[0]:\n",
    "                x_1,y_1,x_2,y_2 = x[0],y[0],x[1],y[1]\n",
    "                y_base,x_base = y[0], x[0] \n",
    "            elif x_val > x[-1]:\n",
    "                x_1,y_1,x_2,y_2=x[-2],y[-2],x[-1],y[-1]\n",
    "                y_base,x_base = y[-1],x[-1]\n",
    "            else:\n",
    "                ind = torch.searchsorted(x, x_val)\n",
    "                if ind==14:\n",
    "                    print(x)\n",
    "                    print(y)\n",
    "                    print(x_val)\n",
    "                x_1,y_1,x_2,y_2 = x[ind-1],y[ind-1],x[ind],y[ind]\n",
    "                y_base,x_base=y[ind-1],x[ind-1]\n",
    "\n",
    "\n",
    "            y_target[i] = y_base + (x_val - x_base) * (y_2-y_1)/(x_2-x_1)\n",
    "\n",
    "        return y_target\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "startgrid=torch.tensor([1.0e-7,1,2,3,4,5,6,7,8,9,10,k+100])\n",
    "\n",
    "ind=torch.searchsorted(startgrid, k)\n",
    "grid=torch.cat((startgrid[:ind],torch.tensor([k*0.99999, k]),\n",
    "                        startgrid[ind:]))\n",
    "\n",
    "\n",
    "\n",
    "v_array =grid/3\n",
    "print(type(v_array))\n",
    "print(type(grid))\n",
    "v = interp1d(grid, v_array, bounds_error=False, \n",
    "            fill_value=\"extrapolate\")\n",
    "print(v(1.1),v(1600))\n",
    "\n",
    "\n",
    "v = torchInterp1d(grid, v_array)\n",
    "print(v(torch.tensor([1.1,1600])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint1\n",
      "checkpoint2\n",
      "Epoch: 01/30 Loss: 9.51646e+03\n",
      "Epoch: 02/30 Loss: 9.51646e+03\n",
      "Epoch: 03/30 Loss: 9.51645e+03\n",
      "Epoch: 04/30 Loss: 9.51645e+03\n",
      "Epoch: 05/30 Loss: 9.51644e+03\n",
      "Epoch: 06/30 Loss: 9.51644e+03\n",
      "Epoch: 07/30 Loss: 9.51643e+03\n",
      "Epoch: 08/30 Loss: 9.51643e+03\n",
      "Epoch: 09/30 Loss: 9.51642e+03\n",
      "Epoch: 10/30 Loss: 9.51642e+03\n",
      "Epoch: 11/30 Loss: 9.51641e+03\n",
      "Epoch: 12/30 Loss: 9.51641e+03\n",
      "Epoch: 13/30 Loss: 9.51640e+03\n",
      "Epoch: 14/30 Loss: 9.51640e+03\n",
      "Epoch: 15/30 Loss: 9.51639e+03\n",
      "Epoch: 16/30 Loss: 9.51639e+03\n",
      "Epoch: 17/30 Loss: 9.51638e+03\n",
      "Epoch: 18/30 Loss: 9.51638e+03\n",
      "Epoch: 19/30 Loss: 9.51637e+03\n",
      "Epoch: 20/30 Loss: 9.51637e+03\n",
      "Epoch: 21/30 Loss: 9.51636e+03\n",
      "Epoch: 22/30 Loss: 9.51636e+03\n",
      "Epoch: 23/30 Loss: 9.51635e+03\n",
      "Epoch: 24/30 Loss: 9.51635e+03\n",
      "Epoch: 25/30 Loss: 9.51634e+03\n",
      "Epoch: 26/30 Loss: 9.51634e+03\n",
      "Epoch: 27/30 Loss: 9.51634e+03\n",
      "Epoch: 28/30 Loss: 9.51633e+03\n",
      "Epoch: 29/30 Loss: 9.51633e+03\n",
      "Epoch: 30/30 Loss: 9.51632e+03\n",
      "tensor([ 1.0000,  0.3333,  0.3333,  0.3333,  0.6667,  1.0000,  1.3333,  1.6667,\n",
      "         2.0000,  2.3333,  2.6667,  3.0000,  3.3333, 33.6666],\n",
      "       requires_grad=True)\n",
      "tensor([ 6.9386e+02,  1.1812e+00,  1.1811e+00,  1.1808e+00, -5.8521e-02,\n",
      "        -7.0947e-01, -1.1444e+00, -1.4487e+00, -1.6993e+00, -1.8937e+00,\n",
      "        -2.0646e+00, -2.2063e+00, -2.3355e+00, -4.1064e+00],\n",
      "       grad_fn=<NegBackward0>)\n",
      "checkpoint3\n",
      "tensor(1.1814, grad_fn=<SelectBackward0>)\n",
      "checkpoint1\n",
      "checkpoint2\n",
      "Epoch: 01/30 Loss: -1.60943e+04\n",
      "Epoch: 02/30 Loss: -1.60944e+04\n",
      "Epoch: 03/30 Loss: -1.60944e+04\n",
      "Epoch: 04/30 Loss: -1.60944e+04\n",
      "Epoch: 05/30 Loss: -1.60944e+04\n",
      "Epoch: 06/30 Loss: -1.60944e+04\n",
      "Epoch: 07/30 Loss: -1.60945e+04\n",
      "Epoch: 08/30 Loss: -1.60945e+04\n",
      "Epoch: 09/30 Loss: -1.60945e+04\n",
      "Epoch: 10/30 Loss: -1.60945e+04\n",
      "Epoch: 11/30 Loss: -1.60945e+04\n",
      "Epoch: 12/30 Loss: -1.60946e+04\n",
      "Epoch: 13/30 Loss: -1.60946e+04\n",
      "Epoch: 14/30 Loss: -1.60946e+04\n",
      "Epoch: 15/30 Loss: -1.60946e+04\n",
      "Epoch: 16/30 Loss: -1.60947e+04\n",
      "Epoch: 17/30 Loss: -1.60947e+04\n",
      "Epoch: 18/30 Loss: -1.60947e+04\n",
      "Epoch: 19/30 Loss: -1.60947e+04\n",
      "Epoch: 20/30 Loss: -1.60947e+04\n",
      "Epoch: 21/30 Loss: -1.60948e+04\n",
      "Epoch: 22/30 Loss: -1.60948e+04\n",
      "Epoch: 23/30 Loss: -1.60948e+04\n",
      "Epoch: 24/30 Loss: -1.60948e+04\n",
      "Epoch: 25/30 Loss: -1.60948e+04\n",
      "Epoch: 26/30 Loss: -1.60949e+04\n",
      "Epoch: 27/30 Loss: -1.60949e+04\n",
      "Epoch: 28/30 Loss: -1.60949e+04\n",
      "Epoch: 29/30 Loss: -1.60949e+04\n",
      "Epoch: 30/30 Loss: -1.60949e+04\n",
      "tensor([ 1.0001,  0.3333,  0.3333,  0.3333,  0.6667,  1.0000,  1.3333,  1.6667,\n",
      "         2.0000,  2.3333,  2.6667,  3.0000,  3.3333, 33.6666],\n",
      "       requires_grad=True)\n",
      "tensor([-1.1557e+03,  4.1831e-01,  4.1831e-01,  4.1819e-01,  2.7910e-01,\n",
      "         2.9688e-01,  3.5574e-01,  3.8901e-01,  4.5263e-01,  4.8453e-01,\n",
      "         5.3156e-01,  5.7222e-01,  6.2205e-01,  7.9656e-01],\n",
      "       grad_fn=<NegBackward0>)\n",
      "checkpoint3\n",
      "tensor(0.7629, grad_fn=<SelectBackward0>)\n",
      "checkpoint1\n",
      "checkpoint2\n",
      "Epoch: 01/30 Loss: 2.68647e+04\n",
      "Epoch: 02/30 Loss: 2.68646e+04\n",
      "Epoch: 03/30 Loss: 2.68646e+04\n",
      "Epoch: 04/30 Loss: 2.68644e+04\n",
      "Epoch: 05/30 Loss: 2.68644e+04\n",
      "Epoch: 06/30 Loss: 2.68643e+04\n",
      "Epoch: 07/30 Loss: 2.68641e+04\n",
      "Epoch: 08/30 Loss: 2.68641e+04\n",
      "Epoch: 09/30 Loss: 2.68640e+04\n",
      "Epoch: 10/30 Loss: 2.68639e+04\n",
      "Epoch: 11/30 Loss: 2.68638e+04\n",
      "Epoch: 12/30 Loss: 2.68637e+04\n",
      "Epoch: 13/30 Loss: 2.68636e+04\n",
      "Epoch: 14/30 Loss: 2.68635e+04\n",
      "Epoch: 15/30 Loss: 2.68634e+04\n",
      "Epoch: 16/30 Loss: 2.68633e+04\n",
      "Epoch: 17/30 Loss: 2.68632e+04\n",
      "Epoch: 18/30 Loss: 2.68631e+04\n",
      "Epoch: 19/30 Loss: 2.68630e+04\n",
      "Epoch: 20/30 Loss: 2.68629e+04\n",
      "Epoch: 21/30 Loss: 2.68628e+04\n",
      "Epoch: 22/30 Loss: 2.68627e+04\n",
      "Epoch: 23/30 Loss: 2.68626e+04\n",
      "Epoch: 24/30 Loss: 2.68625e+04\n",
      "Epoch: 25/30 Loss: 2.68624e+04\n",
      "Epoch: 26/30 Loss: 2.68623e+04\n",
      "Epoch: 27/30 Loss: 2.68622e+04\n",
      "Epoch: 28/30 Loss: 2.68621e+04\n",
      "Epoch: 29/30 Loss: 2.68620e+04\n",
      "Epoch: 30/30 Loss: 2.68619e+04\n",
      "tensor([ 0.9997,  0.3333,  0.3333,  0.3333,  0.6667,  1.0000,  1.3333,  1.6667,\n",
      "         2.0000,  2.3333,  2.6667,  3.0000,  3.3333, 33.6666],\n",
      "       requires_grad=True)\n",
      "tensor([ 1.9263e+03,  9.3107e-01,  9.3106e-01,  9.3080e-01,  1.4270e-01,\n",
      "        -2.7174e-01, -5.5649e-01, -7.8117e-01, -9.6259e-01, -1.1020e+00,\n",
      "        -1.2205e+00, -1.3390e+00, -1.4435e+00, -2.8375e+00],\n",
      "       grad_fn=<NegBackward0>)\n",
      "checkpoint3\n",
      "tensor(0.5128, grad_fn=<SelectBackward0>)\n",
      "checkpoint1\n",
      "checkpoint2\n",
      "Epoch: 01/30 Loss: -4.49244e+04\n",
      "Epoch: 02/30 Loss: -4.49246e+04\n",
      "Epoch: 03/30 Loss: -4.49249e+04\n",
      "Epoch: 04/30 Loss: -4.49251e+04\n",
      "Epoch: 05/30 Loss: -4.49253e+04\n",
      "Epoch: 06/30 Loss: -4.49256e+04\n",
      "Epoch: 07/30 Loss: -4.49258e+04\n",
      "Epoch: 08/30 Loss: -4.49260e+04\n",
      "Epoch: 09/30 Loss: -4.49263e+04\n",
      "Epoch: 10/30 Loss: -4.49265e+04\n",
      "Epoch: 11/30 Loss: -4.49268e+04\n",
      "Epoch: 12/30 Loss: -4.49270e+04\n",
      "Epoch: 13/30 Loss: -4.49273e+04\n",
      "Epoch: 14/30 Loss: -4.49275e+04\n",
      "Epoch: 15/30 Loss: -4.49277e+04\n",
      "Epoch: 16/30 Loss: -4.49279e+04\n",
      "Epoch: 17/30 Loss: -4.49282e+04\n",
      "Epoch: 18/30 Loss: -4.49284e+04\n",
      "Epoch: 19/30 Loss: -4.49286e+04\n",
      "Epoch: 20/30 Loss: -4.49288e+04\n",
      "Epoch: 21/30 Loss: -4.49291e+04\n",
      "Epoch: 22/30 Loss: -4.49293e+04\n",
      "Epoch: 23/30 Loss: -4.49296e+04\n",
      "Epoch: 24/30 Loss: -4.49298e+04\n",
      "Epoch: 25/30 Loss: -4.49300e+04\n",
      "Epoch: 26/30 Loss: -4.49303e+04\n",
      "Epoch: 27/30 Loss: -4.49305e+04\n",
      "Epoch: 28/30 Loss: -4.49308e+04\n",
      "Epoch: 29/30 Loss: -4.49310e+04\n",
      "Epoch: 30/30 Loss: -4.49312e+04\n",
      "tensor([ 1.0004,  0.3333,  0.3333,  0.3333,  0.6667,  1.0000,  1.3333,  1.6667,\n",
      "         2.0000,  2.3333,  2.6667,  3.0000,  3.3333, 33.6666],\n",
      "       requires_grad=True)\n",
      "tensor([-3.2104e+03,  5.6898e-01,  5.6897e-01,  5.6879e-01,  1.6354e-01,\n",
      "         1.8047e-02, -4.4619e-02, -9.1976e-02, -1.0290e-01, -1.1208e-01,\n",
      "        -1.0397e-01, -9.8336e-02, -8.2970e-02, -2.1055e-01],\n",
      "       grad_fn=<NegBackward0>)\n",
      "checkpoint3\n",
      "tensor(0.3621, grad_fn=<SelectBackward0>)\n",
      "checkpoint1\n",
      "checkpoint2\n",
      "Epoch: 01/30 Loss: 7.48624e+04\n",
      "Epoch: 02/30 Loss: 7.48616e+04\n",
      "Epoch: 03/30 Loss: 7.48608e+04\n",
      "Epoch: 04/30 Loss: 7.48601e+04\n",
      "tensor([1.0000e-07, 9.9979e-01, 9.9980e-01, 1.0000e+00, 2.0000e+00, 3.0000e+00,\n",
      "        4.0000e+00, 5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00,\n",
      "        1.0000e+01, 1.0100e+02])\n",
      "tensor([-3.2104e+03,  5.6898e-01,  5.6897e-01,  5.6879e-01,  1.6354e-01,\n",
      "         1.8047e-02, -4.4619e-02, -9.1976e-02, -1.0290e-01, -1.1208e-01,\n",
      "        -1.0397e-01, -9.8336e-02, -8.2970e-02, -2.1055e-01],\n",
      "       grad_fn=<NegBackward0>)\n",
      "tensor(nan, grad_fn=<UnbindBackward0>)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 14 is out of bounds for dimension 0 with size 14",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 228\u001b[0m\n\u001b[1;32m    226\u001b[0m modelpar\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiscount\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0.95\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepreciation\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0.08\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber_agents\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m9\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtech_gamma\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0.3\u001b[39m,\u001b[38;5;241m0.35\u001b[39m,\u001b[38;5;241m0.45\u001b[39m]),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtech_cost\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0.15\u001b[39m,\u001b[38;5;241m0.65\u001b[39m]),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_nodes\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m9\u001b[39m}        \n\u001b[1;32m    227\u001b[0m ndata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtheta\u001b[39m\u001b[38;5;124m'\u001b[39m:torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0.7603\u001b[39m, \u001b[38;5;241m0.7641\u001b[39m, \u001b[38;5;241m0.9275\u001b[39m, \u001b[38;5;241m0.7906\u001b[39m, \u001b[38;5;241m0.7858\u001b[39m, \u001b[38;5;241m0.7832\u001b[39m, \u001b[38;5;241m0.3904\u001b[39m, \u001b[38;5;241m0.3924\u001b[39m, \u001b[38;5;241m0.7884\u001b[39m]),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigma\u001b[39m\u001b[38;5;124m'\u001b[39m:torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1.3000\u001b[39m, \u001b[38;5;241m1.8000\u001b[39m, \u001b[38;5;241m0.6000\u001b[39m, \u001b[38;5;241m0.7000\u001b[39m, \u001b[38;5;241m1.2000\u001b[39m, \u001b[38;5;241m1.1000\u001b[39m, \u001b[38;5;241m0.7000\u001b[39m, \u001b[38;5;241m1.8000\u001b[39m, \u001b[38;5;241m1.4000\u001b[39m]),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m:torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0.9952\u001b[39m, \u001b[38;5;241m1.1045\u001b[39m, \u001b[38;5;241m1.0706\u001b[39m, \u001b[38;5;241m1.0489\u001b[39m, \u001b[38;5;241m1.0576\u001b[39m, \u001b[38;5;241m1.1922\u001b[39m, \u001b[38;5;241m1.0162\u001b[39m, \u001b[38;5;241m1.0938\u001b[39m, \u001b[38;5;241m1.1324\u001b[39m]),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwealth\u001b[39m\u001b[38;5;124m'\u001b[39m:torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0.9998\u001b[39m, \u001b[38;5;241m1.1309\u001b[39m, \u001b[38;5;241m1.1439\u001b[39m, \u001b[38;5;241m1.0342\u001b[39m, \u001b[38;5;241m1.2321\u001b[39m, \u001b[38;5;241m1.0819\u001b[39m, \u001b[38;5;241m1.0326\u001b[39m, \u001b[38;5;241m1.1193\u001b[39m, \u001b[38;5;241m1.0707\u001b[39m]), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma_table\u001b[39m\u001b[38;5;124m'\u001b[39m:torch\u001b[38;5;241m.\u001b[39mstack([torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0.5\u001b[39m,\u001b[38;5;241m0.9\u001b[39m]),torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0.25\u001b[39m,\u001b[38;5;241m0.45\u001b[39m])])\u001b[38;5;241m.\u001b[39mrepeat(modelpar[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber_agents\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)}\n\u001b[0;32m--> 228\u001b[0m \u001b[43m_torch_optimized_wealth_consumption\u001b[49m\u001b[43m(\u001b[49m\u001b[43mndata\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodelpar\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 223\u001b[0m, in \u001b[0;36m_torch_optimized_wealth_consumption\u001b[0;34m(model_graph, model_params)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(model_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber_agents\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m    222\u001b[0m     agentinfo \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m:utility, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m:income_function, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mβ\u001b[39m\u001b[38;5;124m'\u001b[39m:model_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiscount\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mθ\u001b[39m\u001b[38;5;124m'\u001b[39m:model_graph[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtheta\u001b[39m\u001b[38;5;124m'\u001b[39m][i], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m𝛿\u001b[39m\u001b[38;5;124m'\u001b[39m:model_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepreciation\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mσ\u001b[39m\u001b[38;5;124m'\u001b[39m:model_graph[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigma\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\u001b[38;5;241m.\u001b[39mnumpy(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mα\u001b[39m\u001b[38;5;124m'\u001b[39m: model_graph[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m][i],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m:model_graph[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwealth\u001b[39m\u001b[38;5;124m'\u001b[39m][i],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madapt\u001b[39m\u001b[38;5;124m'\u001b[39m: model_graph[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma_table\u001b[39m\u001b[38;5;124m'\u001b[39m][i]}\n\u001b[0;32m--> 223\u001b[0m     model_graph[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwealth_consumption\u001b[39m\u001b[38;5;124m'\u001b[39m][i], model_graph[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi_a\u001b[39m\u001b[38;5;124m'\u001b[39m][i], model_graph[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m'\u001b[39m][i] \u001b[38;5;241m=\u001b[39m \u001b[43mwhich_bellman\u001b[49m\u001b[43m(\u001b[49m\u001b[43magentinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_graph)\n",
      "Cell \u001b[0;32mIn[13], line 165\u001b[0m, in \u001b[0;36m_torch_optimized_wealth_consumption.<locals>.which_bellman\u001b[0;34m(agentinfo)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;66;03m#  print(f'working theta = {agentinfo.θ + option[0] *\\\u001b[39;00m\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m#  (1-agentinfo.θ)}, i_a= {option[1]}, k= {agentinfo.k}')\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m         c,v\u001b[38;5;241m=\u001b[39m\u001b[43msolve_bellman\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTensBellmanEquation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutility\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mincome_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magentinfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mk\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mθ\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magentinfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mθ\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mσ\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magentinfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mσ\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mα\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magentinfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mα\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mβ\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiscount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m𝛿\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdepreciation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtech\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgamma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtech_gamma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtech_cost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m         feasible\u001b[38;5;241m.\u001b[39mappend([v,c,option[\u001b[38;5;241m1\u001b[39m],option[\u001b[38;5;241m0\u001b[39m]])\n\u001b[1;32m    173\u001b[0m best\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(feasible)\n",
      "Cell \u001b[0;32mIn[13], line 197\u001b[0m, in \u001b[0;36m_torch_optimized_wealth_consumption.<locals>.solve_bellman\u001b[0;34m(bell, tol, min_iter, max_iter, verbose)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (i \u001b[38;5;241m<\u001b[39m max_iter \u001b[38;5;129;01mand\u001b[39;00m error \u001b[38;5;241m>\u001b[39m tol) \u001b[38;5;129;01mor\u001b[39;00m (i \u001b[38;5;241m<\u001b[39m min_iter):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 197\u001b[0m     v_greedy, v_new \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_bellman\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    199\u001b[0m     error \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(v[bell\u001b[38;5;241m.\u001b[39mindex] \u001b[38;5;241m-\u001b[39m v_new)[bell\u001b[38;5;241m.\u001b[39mindex]\n",
      "Cell \u001b[0;32mIn[13], line 137\u001b[0m, in \u001b[0;36m_torch_optimized_wealth_consumption.<locals>.update_bellman\u001b[0;34m(v, bell)\u001b[0m\n\u001b[1;32m    132\u001b[0m b\u001b[38;5;241m=\u001b[39m(y\u001b[38;5;241m-\u001b[39mbell\u001b[38;5;241m.\u001b[39mi_a)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m#pairs = torch.transpose(torch.stack((a,b),0),1,0).tolist()\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m#boundsequence = [tuple(x) for x in pairs]\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m c_star, v_max \u001b[38;5;241m=\u001b[39m \u001b[43mtorchmaximize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m#VMG HELP! can anyone check that (1) subtracting i_a and \u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# (2) omitting any grid values less than i_a \u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# will not be problematic? The only thing I can come up with\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# is if i_a is greater than k*0.99999\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# which_bellman() now accounts for that case. Whole thing \u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# could use refinement.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m v_new \u001b[38;5;241m=\u001b[39m v_max\n",
      "Cell \u001b[0;32mIn[13], line 87\u001b[0m, in \u001b[0;36m_torch_optimized_wealth_consumption.<locals>.torchmaximize\u001b[0;34m(g, a, b, args)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(x):  \u001b[38;5;66;03m# You can adjust the number of optimization steps\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m#history.append(g(x,*args))\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep(closure)\n\u001b[0;32m---> 87\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m30\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 67\u001b[0m, in \u001b[0;36m_torch_optimized_wealth_consumption.<locals>.torchmaximize.<locals>.closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m():\n\u001b[1;32m     66\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 67\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     loss\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(value)\n\u001b[1;32m     69\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[13], line 44\u001b[0m, in \u001b[0;36mTensBellmanEquation.value\u001b[0;34m(self, c, y, v_array)\u001b[0m\n\u001b[1;32m     40\u001b[0m u, f, β, θ, 𝛿, σ, α, i_a, m, tech \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mβ, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mθ, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m𝛿, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mσ, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mα, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi_a, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtech\n\u001b[1;32m     42\u001b[0m v \u001b[38;5;241m=\u001b[39m torchInterp1d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid, v_array)\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m u(c,σ) \u001b[38;5;241m+\u001b[39m β \u001b[38;5;241m*\u001b[39m \u001b[43mv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mθ\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mθ\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mα\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtech\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi_a\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m𝛿\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 29\u001b[0m, in \u001b[0;36mtorchInterp1d.__call__\u001b[0;34m(self, x_target)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28mprint\u001b[39m(y)\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28mprint\u001b[39m(x_val)\n\u001b[0;32m---> 29\u001b[0m     x_1,y_1,x_2,y_2 \u001b[38;5;241m=\u001b[39m x[ind\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],y[ind\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mind\u001b[49m\u001b[43m]\u001b[49m,y[ind]\n\u001b[1;32m     30\u001b[0m     y_base,x_base\u001b[38;5;241m=\u001b[39my[ind\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],x[ind\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     33\u001b[0m y_target[i] \u001b[38;5;241m=\u001b[39m y_base \u001b[38;5;241m+\u001b[39m (x_val \u001b[38;5;241m-\u001b[39m x_base) \u001b[38;5;241m*\u001b[39m (y_2\u001b[38;5;241m-\u001b[39my_1)\u001b[38;5;241m/\u001b[39m(x_2\u001b[38;5;241m-\u001b[39mx_1)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 14 is out of bounds for dimension 0 with size 14"
     ]
    }
   ],
   "source": [
    "\n",
    "def income_function(k,α,tech): \n",
    "    f=α * k.unsqueeze(-1)**tech['gamma'] - tech['cost']\n",
    "    return torch.max(f,-1)[0]\n",
    "\n",
    "class TensBellmanEquation:\n",
    "    #Adapted from: https://python.quantecon.org/optgrowth.html\n",
    "    def __init__(self,\n",
    "                u,            # utility function\n",
    "                f,            # production function\n",
    "                k,            # current state k_t\n",
    "                θ,            # given shock factor θ\n",
    "                σ,            # risk averseness\n",
    "                α,            # human capital\n",
    "                i_a,          # adaptation investment\n",
    "                m,            # protection multiplier\n",
    "                β,            # discount factor\n",
    "                𝛿,            # depreciation factor \n",
    "                tech):       # adaptation table \n",
    "                #name=\"BellmanNarrowExtended\"\n",
    "                \n",
    "\n",
    "        self.u, self.f, self.k, self.β, self.θ, self.𝛿, self.σ, self.α, self.i_a, self.m, self.tech = u, f, k, β, θ, 𝛿, σ, α, i_a, m, tech\n",
    "\n",
    "        # Set up grid\n",
    "        \n",
    "        startgrid=torch.tensor([1.0e-7,1,2,3,4,5,6,7,8,9,10,k+100])\n",
    "\n",
    "        ind=torch.searchsorted(startgrid, k)\n",
    "        self.grid=torch.cat((startgrid[:ind],torch.tensor([k*0.99999, k]),\n",
    "                                startgrid[ind:]))\n",
    "\n",
    "        self.grid=self.grid[self.grid>i_a]\n",
    "        # Identify target state k\n",
    "        self.index = torch.searchsorted(self.grid, k)-1\n",
    "    def value(self, c, y, v_array):\n",
    "        \"\"\"\n",
    "        Right hand side of the Bellman equation.\n",
    "        \"\"\"\n",
    "\n",
    "        u, f, β, θ, 𝛿, σ, α, i_a, m, tech = self.u, self.f, self.β, self.θ, self.𝛿, self.σ, self.α, self.i_a, self.m, self.tech\n",
    "\n",
    "        v = torchInterp1d(self.grid, v_array)\n",
    "\n",
    "        return u(c,σ) + β * v((θ + m * (1-θ)) * (f(y,α,tech) - c - i_a + (1 - 𝛿) * y))\n",
    "\n",
    "\n",
    "\n",
    "def _torch_optimized_wealth_consumption(model_graph, model_params):\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def torchmaximize(g, a, b, args):\n",
    "        \"\"\"\n",
    "        Maximize the function g over the interval [a, b] using PyTorch's LBFGS optimizer.\n",
    "\n",
    "        The tuple args collects any extra arguments to g.\n",
    "\n",
    "        Returns the maximal value and the maximizer.\n",
    "        \"\"\"\n",
    "\n",
    "        # Define a closure function  to reset gradient, determine value, and calculate new gradient\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            value = -g(x, *args)\n",
    "            loss= torch.sum(value)\n",
    "            loss.backward(retain_graph=True)\n",
    "            return loss\n",
    "\n",
    "        # Initial guess for consumption\n",
    "        x = (args[0]/3).clone().requires_grad_(True)\n",
    "\n",
    "        # Feed it to optimizer and keep x guesses in the bounds\n",
    "        optimizer = torch.optim.LBFGS([x], history_size=10, max_iter=4)\n",
    "        x.data = torch.clamp(x, a, b)\n",
    "\n",
    "        #history=[]\n",
    "        # Run optimization for 30 epochs:\n",
    "        for epoch in range(30):\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for i, c in enumerate(x):  # You can adjust the number of optimization steps\n",
    "                #history.append(g(x,*args))\n",
    "                optimizer.step(closure)\n",
    "                loss = closure()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch: {epoch + 1:02}/{30} Loss: {running_loss:.5e}\")\n",
    "\n",
    "\n",
    "\n",
    "        maximizer = x\n",
    "        maximum = -g(x, *args)\n",
    "        print(maximizer)\n",
    "        print(maximum)\n",
    "        #print(history)\n",
    "        return maximizer, maximum\n",
    "\n",
    "    def utility(c, σ, type=\"isoelastic\"):\n",
    "        if type == \"isoelastic\":\n",
    "            if σ ==1:\n",
    "                return torch.log(c)\n",
    "            else:\n",
    "                return (c**(1-σ)-1)/(1-σ)\n",
    "\n",
    "        else:\n",
    "            print(\"Unspecified utility function!!!\")\n",
    "\n",
    "\n",
    "    def update_bellman(v, bell):\n",
    "        \"\"\"\n",
    "        From: https://python.quantecon.org/optgrowth.html (similar example\n",
    "        https://macroeconomics.github.io/Dynamic%20Programming.html#.ZC13-exBy3I)\n",
    "        \n",
    "        The Bellman operator.  Updates the guess of the value function\n",
    "        and also computes a v-greedy policy.\n",
    "\n",
    "        * bell is an instance of Bellman equation\n",
    "        * v is an array representing a guess of the value function\n",
    "\n",
    "        \"\"\"\n",
    "        v_new = torch.empty_like(v)\n",
    "        v_greedy = torch.empty_like(v)\n",
    "        \n",
    "        print('checkpoint2')\n",
    "\n",
    "        y = bell.grid\n",
    "        # Maximize RHS of Bellman equation at state y\n",
    "        a=torch.min(torch.transpose(torch.stack((torch.full(y.size(),1e-8),(y*0.00001)),0),0,1),1)[0]\n",
    "        b=(y-bell.i_a)\n",
    "\n",
    "        #pairs = torch.transpose(torch.stack((a,b),0),1,0).tolist()\n",
    "        #boundsequence = [tuple(x) for x in pairs]\n",
    "\n",
    "        c_star, v_max = torchmaximize(bell.value, a, b, (y, v))\n",
    "            #VMG HELP! can anyone check that (1) subtracting i_a and \n",
    "            # (2) omitting any grid values less than i_a \n",
    "            # will not be problematic? The only thing I can come up with\n",
    "            # is if i_a is greater than k*0.99999\n",
    "            # which_bellman() now accounts for that case. Whole thing \n",
    "            # could use refinement.\n",
    "\n",
    "        v_new = v_max\n",
    "        v_greedy = c_star\n",
    "\n",
    "        return v_greedy, v_new\n",
    "\n",
    "\n",
    "    def which_bellman(agentinfo):\n",
    "        \"\"\"\n",
    "        Solves bellman for each affordable adaptation option.\n",
    "        \"\"\"\n",
    "        feasible=[]\n",
    "\n",
    "        for option in torch.transpose(agentinfo['adapt'],0,1):\n",
    "            if option[1]>=(income_function(agentinfo['k'],agentinfo['α'],tech={'gamma':model_params['tech_gamma'],'cost':model_params['tech_cost']})+(1 - model_params['depreciation'])*agentinfo['k'])*.99998:\n",
    "                # ensures that the gridpoint\n",
    "                # just below k, k*0.99999, is included\n",
    "                pass\n",
    "            else:\n",
    "                #  print(f'working theta = {agentinfo.θ + option[0] *\\\n",
    "                #  (1-agentinfo.θ)}, i_a= {option[1]}, k= {agentinfo.k}')\n",
    "                c,v=solve_bellman(TensBellmanEquation(u=utility, \n",
    "                                f=income_function, k=agentinfo['k'], \n",
    "                                θ=agentinfo['θ'], σ=agentinfo['σ'], \n",
    "                                α=agentinfo['α'], i_a=option[1],m=option[0],\n",
    "                                β=model_params['discount'], 𝛿=model_params['depreciation'],\n",
    "                                tech={'gamma':model_params['tech_gamma'],'cost':model_params['tech_cost']}))\n",
    "                feasible.append([v,c,option[1],option[0]])\n",
    "\n",
    "        best=min(feasible)\n",
    "\n",
    "        return best[1],best[2],best[3]\n",
    "\n",
    "    def solve_bellman(bell,\n",
    "                    tol=1,\n",
    "                    min_iter=10,\n",
    "                    max_iter=1000,\n",
    "                    verbose=False):\n",
    "        \"\"\"\n",
    "        From: https://python.quantecon.org/optgrowth.html (similar example\n",
    "        https://macroeconomics.github.io/Dynamic%20Programming.html#.ZC13-exBy3I)\n",
    "        \n",
    "        Solve model by iterating with the Bellman operator.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Set up loop\n",
    "\n",
    "        v = bell.u(bell.grid,bell.σ)  # Initial condition\n",
    "        i = 0\n",
    "        error = tol + 1\n",
    "        while (i < max_iter and error > tol) or (i < min_iter):\n",
    "            print('checkpoint1')\n",
    "            v_greedy, v_new = update_bellman(v, bell)\n",
    "            print('checkpoint3')\n",
    "            error = torch.abs(v[bell.index] - v_new)[bell.index]\n",
    "            print(error)\n",
    "            i += 1\n",
    "            # if verbose and i % print_skip == 0:\n",
    "            #     print(f\"Error at iteration {i} is {error}.\")\n",
    "            v = v_new\n",
    "\n",
    "        print('checkpoint4')\n",
    "\n",
    "        if error > tol:\n",
    "            print(f\"{bell.name} failed to converge for k={bell.k}, α = {bell.α},σ ={bell.σ}, i_a={bell.i_a}, and modified θ = {bell.θ + bell.m * (1-bell.θ)}!\")\n",
    "        elif verbose:\n",
    "            print(f\"Converged in {i} iterations.\")\n",
    "            print(f\"Effective k and new c {torch.round(bell.grid[bell.index],3),v_greedy[bell.index]}\")\n",
    "            \n",
    "\n",
    "        return v_greedy[bell.index],v[bell.index]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    model_graph['wealth_consumption'], model_graph['i_a'], model_graph['m'] = torch.zeros(9),torch.zeros(9),torch.zeros(9)\n",
    "    for i in range(model_params['number_agents']):\n",
    "        agentinfo = {'u':utility, 'f':income_function, 'β':model_params['discount'], 'θ':model_graph['theta'][i], '𝛿':model_params['depreciation'], 'σ':model_graph['sigma'][i].numpy(), 'α': model_graph['alpha'][i],'k':model_graph['wealth'][i],'adapt': model_graph['a_table'][i]}\n",
    "        model_graph['wealth_consumption'][i], model_graph['i_a'][i], model_graph['m'][i] = which_bellman(agentinfo)\n",
    "    print(model_graph)\n",
    "\n",
    "modelpar={'discount':0.95,'depreciation':0.08,'number_agents':9,'tech_gamma': torch.tensor([0.3,0.35,0.45]),'tech_cost': torch.tensor([0,0.15,0.65]),'num_nodes':9}        \n",
    "ndata={'theta':torch.tensor([0.7603, 0.7641, 0.9275, 0.7906, 0.7858, 0.7832, 0.3904, 0.3924, 0.7884]),'sigma':torch.tensor([1.3000, 1.8000, 0.6000, 0.7000, 1.2000, 1.1000, 0.7000, 1.8000, 1.4000]),'alpha':torch.tensor([0.9952, 1.1045, 1.0706, 1.0489, 1.0576, 1.1922, 1.0162, 1.0938, 1.1324]),'wealth':torch.tensor([0.9998, 1.1309, 1.1439, 1.0342, 1.2321, 1.0819, 1.0326, 1.1193, 1.0707]), 'a_table':torch.stack([torch.tensor([0,0.5,0.9]),torch.tensor([0,0.25,0.45])]).repeat(modelpar['number_agents'],1,1)}\n",
    "_torch_optimized_wealth_consumption(ndata,modelpar)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base=torch.tensor([1.0000e-07, 1.0100e+02])\n",
    "coef=torch.tensor(0.9952)\n",
    "powers=torch.tensor([0.3000, 0.3500, 0.4500])\n",
    "cost=torch.tensor([0,0.15,0.65])\n",
    "#powers=torch.transpose(powers,-1,0)#.repeat(4,1)\n",
    "print(powers.dtype)\n",
    "print(base.dtype)\n",
    "print(coef.dtype)\n",
    "r1=coef * base[:, None] ** powers - cost\n",
    "torch.max(r1,-1)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def income_function(k,α,tech): \n",
    "    f=α * k.unsqueeze(-1)**tech['gamma'] - tech['cost']\n",
    "    return torch.max(f,-1)[0]\n",
    "\n",
    "class TensBellmanEquation:\n",
    "    #Adapted from: https://python.quantecon.org/optgrowth.html\n",
    "    def __init__(self,\n",
    "                u,            # utility function\n",
    "                f,            # production function\n",
    "                k,            # current state k_t\n",
    "                θ,            # given shock factor θ\n",
    "                σ,            # risk averseness\n",
    "                α,            # human capital\n",
    "                i_a,          # adaptation investment\n",
    "                m,            # protection multiplier\n",
    "                β,            # discount factor\n",
    "                𝛿,            # depreciation factor \n",
    "                tech):       # adaptation table \n",
    "                #name=\"BellmanNarrowExtended\"\n",
    "                \n",
    "\n",
    "        self.u, self.f, self.k, self.β, self.θ, self.𝛿, self.σ, self.α, self.i_a, self.m, self.tech = u, f, k, β, θ, 𝛿, σ, α, i_a, m, tech\n",
    "\n",
    "        # Set up grid\n",
    "        \n",
    "        startgrid=torch.tensor([1.0e-7,1,2,3,4,5,6,7,8,9,10,k+100])\n",
    "\n",
    "        ind=torch.searchsorted(startgrid, k)\n",
    "        self.grid=torch.cat((startgrid[:ind],torch.tensor([k*0.99999, k]),\n",
    "                                startgrid[ind:]))\n",
    "\n",
    "        self.grid=self.grid[self.grid>i_a]\n",
    "        # Identify target state k\n",
    "        self.index = torch.searchsorted(self.grid, k)-1\n",
    "    def value(self, c, y, v_array):\n",
    "        \"\"\"\n",
    "        Right hand side of the Bellman equation.\n",
    "        \"\"\"\n",
    "\n",
    "        u, f, β, θ, 𝛿, σ, α, i_a, m, tech = self.u, self.f, self.β, self.θ, self.𝛿, self.σ, self.α, self.i_a, self.m, self.tech\n",
    "\n",
    "        v = torchInterp1d(self.grid, v_array)\n",
    "\n",
    "        return u(c,σ) + β * v((θ + m * (1-θ)) * (f(y,α,tech) - c - i_a + (1 - 𝛿) * y))\n",
    "\n",
    "\n",
    "\n",
    "def _torch_optimized_wealth_consumption(model_graph, model_params):\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def torchmaximize(g, a, b, args):\n",
    "        \"\"\"\n",
    "        Maximize the function g over the interval [a, b] using PyTorch's LBFGS optimizer.\n",
    "\n",
    "        The tuple args collects any extra arguments to g.\n",
    "\n",
    "        Returns the maximal value and the maximizer.\n",
    "        \"\"\"\n",
    "\n",
    "        # Define a closure function  to reset gradient, determine value, and calculate new gradient\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            value = -g(x, *args)\n",
    "            loss= torch.sum(value)\n",
    "            loss.backward(retain_graph=True)\n",
    "            return loss\n",
    "\n",
    "        # Initial guess for consumption\n",
    "        x = (args[0]/3).clone().requires_grad_(True)\n",
    "\n",
    "        # Feed it to optimizer and keep x guesses in the bounds\n",
    "        optimizer = torch.optim.LBFGS([x], history_size=10, max_iter=4)\n",
    "        x.data = torch.clamp(x, a, b)\n",
    "\n",
    "        #history=[]\n",
    "        # Run optimization for 30 epochs:\n",
    "        for epoch in range(30):\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for i, c in enumerate(x):  # You can adjust the number of optimization steps\n",
    "                #history.append(g(x,*args))\n",
    "                optimizer.step(closure)\n",
    "                loss = closure()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch: {epoch + 1:02}/{30} Loss: {running_loss:.5e}\")\n",
    "\n",
    "\n",
    "\n",
    "        maximizer = x\n",
    "        maximum = -g(x, *args)\n",
    "        print(maximizer)\n",
    "        print(maximum)\n",
    "        #print(history)\n",
    "        return maximizer, maximum\n",
    "\n",
    "    def utility(c, σ, type=\"isoelastic\"):\n",
    "        if type == \"isoelastic\":\n",
    "            if σ ==1:\n",
    "                return torch.log(c)\n",
    "            else:\n",
    "                return (c**(1-σ)-1)/(1-σ)\n",
    "\n",
    "        else:\n",
    "            print(\"Unspecified utility function!!!\")\n",
    "\n",
    "\n",
    "    def update_bellman(v, bell):\n",
    "        \"\"\"\n",
    "        From: https://python.quantecon.org/optgrowth.html (similar example\n",
    "        https://macroeconomics.github.io/Dynamic%20Programming.html#.ZC13-exBy3I)\n",
    "        \n",
    "        The Bellman operator.  Updates the guess of the value function\n",
    "        and also computes a v-greedy policy.\n",
    "\n",
    "        * bell is an instance of Bellman equation\n",
    "        * v is an array representing a guess of the value function\n",
    "\n",
    "        \"\"\"\n",
    "        v_new = torch.empty_like(v)\n",
    "        v_greedy = torch.empty_like(v)\n",
    "        \n",
    "        print('checkpoint2')\n",
    "\n",
    "        y = bell.grid\n",
    "        # Maximize RHS of Bellman equation at state y\n",
    "        a=torch.min(torch.transpose(torch.stack((torch.full(y.size(),1e-8),(y*0.00001)),0),0,1),1)[0]\n",
    "        b=(y-bell.i_a)\n",
    "\n",
    "        #pairs = torch.transpose(torch.stack((a,b),0),1,0).tolist()\n",
    "        #boundsequence = [tuple(x) for x in pairs]\n",
    "\n",
    "        c_star, v_max = torchmaximize(bell.value, a, b, (y, v))\n",
    "            #VMG HELP! can anyone check that (1) subtracting i_a and \n",
    "            # (2) omitting any grid values less than i_a \n",
    "            # will not be problematic? The only thing I can come up with\n",
    "            # is if i_a is greater than k*0.99999\n",
    "            # which_bellman() now accounts for that case. Whole thing \n",
    "            # could use refinement.\n",
    "\n",
    "        v_new = v_max\n",
    "        v_greedy = c_star\n",
    "\n",
    "        return v_greedy, v_new\n",
    "\n",
    "\n",
    "    def which_bellman(agentinfo):\n",
    "        \"\"\"\n",
    "        Solves bellman for each affordable adaptation option.\n",
    "        \"\"\"\n",
    "        feasible=[]\n",
    "\n",
    "        for option in torch.transpose(agentinfo['adapt'],0,1):\n",
    "            if option[1]>=(income_function(agentinfo['k'],agentinfo['α'],tech={'gamma':model_params['tech_gamma'],'cost':model_params['tech_cost']})+(1 - model_params['depreciation'])*agentinfo['k'])*.99998:\n",
    "                # ensures that the gridpoint\n",
    "                # just below k, k*0.99999, is included\n",
    "                pass\n",
    "            else:\n",
    "                #  print(f'working theta = {agentinfo.θ + option[0] *\\\n",
    "                #  (1-agentinfo.θ)}, i_a= {option[1]}, k= {agentinfo.k}')\n",
    "                c,v=solve_bellman(TensBellmanEquation(u=utility, \n",
    "                                f=income_function, k=agentinfo['k'], \n",
    "                                θ=agentinfo['θ'], σ=agentinfo['σ'], \n",
    "                                α=agentinfo['α'], i_a=option[1],m=option[0],\n",
    "                                β=model_params['discount'], 𝛿=model_params['depreciation'],\n",
    "                                tech={'gamma':model_params['tech_gamma'],'cost':model_params['tech_cost']}))\n",
    "                feasible.append([v,c,option[1],option[0]])\n",
    "\n",
    "        best=min(feasible)\n",
    "\n",
    "        return best[1],best[2],best[3]\n",
    "\n",
    "    def solve_bellman(bell,\n",
    "                    tol=1,\n",
    "                    min_iter=10,\n",
    "                    max_iter=1000,\n",
    "                    verbose=False):\n",
    "        \"\"\"\n",
    "        From: https://python.quantecon.org/optgrowth.html (similar example\n",
    "        https://macroeconomics.github.io/Dynamic%20Programming.html#.ZC13-exBy3I)\n",
    "        \n",
    "        Solve model by iterating with the Bellman operator.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Set up loop\n",
    "\n",
    "        v = bell.u(bell.grid,bell.σ)  # Initial condition\n",
    "        i = 0\n",
    "        error = tol + 1\n",
    "        while (i < max_iter and error > tol) or (i < min_iter):\n",
    "            print('checkpoint1')\n",
    "            v_greedy, v_new = update_bellman(v, bell)\n",
    "            print('checkpoint3')\n",
    "            error = torch.abs(v[bell.index] - v_new)[bell.index]\n",
    "            print(error)\n",
    "            i += 1\n",
    "            # if verbose and i % print_skip == 0:\n",
    "            #     print(f\"Error at iteration {i} is {error}.\")\n",
    "            v = v_new\n",
    "\n",
    "        print('checkpoint4')\n",
    "\n",
    "        if error > tol:\n",
    "            print(f\"{bell.name} failed to converge for k={bell.k}, α = {bell.α},σ ={bell.σ}, i_a={bell.i_a}, and modified θ = {bell.θ + bell.m * (1-bell.θ)}!\")\n",
    "        elif verbose:\n",
    "            print(f\"Converged in {i} iterations.\")\n",
    "            print(f\"Effective k and new c {torch.round(bell.grid[bell.index],3),v_greedy[bell.index]}\")\n",
    "            \n",
    "\n",
    "        return v_greedy[bell.index],v[bell.index]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    model_graph['wealth_consumption'], model_graph['i_a'], model_graph['m'] = torch.zeros(9),torch.zeros(9),torch.zeros(9)\n",
    "    for i in range(model_params['number_agents']):\n",
    "        agentinfo = {'u':utility, 'f':income_function, 'β':model_params['discount'], 'θ':model_graph['theta'][i], '𝛿':model_params['depreciation'], 'σ':model_graph['sigma'][i].numpy(), 'α': model_graph['alpha'][i],'k':model_graph['wealth'][i],'adapt': model_graph['a_table'][i]}\n",
    "        model_graph['wealth_consumption'][i], model_graph['i_a'][i], model_graph['m'][i] = which_bellman(agentinfo)\n",
    "    print(model_graph)\n",
    "\n",
    "modelpar={'discount':0.95,'depreciation':0.08,'number_agents':9,'tech_gamma': torch.tensor([0.3,0.35,0.45]),'tech_cost': torch.tensor([0,0.15,0.65]),'num_nodes':9}        \n",
    "ndata={'theta':torch.tensor([0.7603, 0.7641, 0.9275, 0.7906, 0.7858, 0.7832, 0.3904, 0.3924, 0.7884]),'sigma':torch.tensor([1.3000, 1.8000, 0.6000, 0.7000, 1.2000, 1.1000, 0.7000, 1.8000, 1.4000]),'alpha':torch.tensor([0.9952, 1.1045, 1.0706, 1.0489, 1.0576, 1.1922, 1.0162, 1.0938, 1.1324]),'wealth':torch.tensor([0.9998, 1.1309, 1.1439, 1.0342, 1.2321, 1.0819, 1.0326, 1.1193, 1.0707]), 'a_table':torch.stack([torch.tensor([0,0.5,0.9]),torch.tensor([0,0.25,0.45])]).repeat(modelpar['number_agents'],1,1)}\n",
    "_torch_optimized_wealth_consumption(ndata,modelpar)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('dgl_ptm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1477468427ea6fe7f3c6460347a373a01cc68daae53faf91f7d9fb578ee805b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
