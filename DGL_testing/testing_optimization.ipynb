{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/victoria/Documents/Scripts/Python/DGL-PTM/dgl_ptm')\n",
    "import dgl_ptm\n",
    "import os\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dgl_ptm.PovertyTrapModel(model_identifier='custom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_model_parameters(default=True)\n",
    "print(model.steering_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dgl_ptm.PovertyTrapModel(model_identifier='testwithnn',restart=False, savestate=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "{'edata': ['all'], 'epath': 'testwithnn/edge_data', 'format': 'xarray', 'mode': 'w', 'ndata': ['all_except', ['a_table']], 'npath': 'testwithnn/agent_data.zarr', 'wealth_method': 'singular_transfer', 'income_method': 'pseudo_income_generation', 'consume_method': 'pseudo_consumption', 'nn_path': '/Users/victoria/Documents/Scripts/Python/DGL-PTM/DGL_testing/nn_data/both_PudgeSixLayer_1024/0723_110813/model_best.pth', 'capital_update_method': 'default', 'characteristic_distance': 35, 'homophily_parameter': 0.69, 'adapt_m': tensor([0.0000, 0.5000, 0.9000]), 'adapt_cost': tensor([0.0000, 0.2500, 0.4500]), 'depreciation': 0.6, 'discount': 0.95, 'm_theta_dist': {'type': 'multinomial', 'parameters': [tensor([0.0200, 0.0300, 0.0500, 0.9000]), tensor([0.7000, 0.8000, 0.9000, 1.0000])], 'round': False, 'decimals': None}, 'tech_gamma': tensor([0.3000, 0.3500, 0.4500]), 'tech_cost': tensor([0.0000, 0.1500, 0.6500]), 'del_method': 'size', 'del_threshold': 0.05, 'noise_ratio': 0.05, 'local_ratio': 0.25, 'truncation_weight': 1e-10, 'step_type': 'custom'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model.set_model_parameters(**{'number_agents': 100 , \n",
    "    'seed':0,\n",
    "    'gamma_vals':torch.tensor([0.3,0.45]) , #for pseudo income\n",
    "    'sigma_dist': {'type':'uniform','parameters':[0.05,1.94],'round':True,'decimals':1},\n",
    "    'cost_vals': torch.tensor([0.,0.45]), #for pseudo income\n",
    "    'a_theta_dist': {'type':'uniform','parameters':[0.1,1],'round':False,'decimals':None},\n",
    "    'sensitivity_dist':{'type':'uniform','parameters':[0.0,1],'round':False,'decimals':None},\n",
    "    'capital_dist': {'type':'uniform','parameters':[0.1,10.],'round':False,'decimals':None}, \n",
    "    'alpha_dist': {'type':'normal','parameters':[1.08,0.074],'round':False,'decimals':None},\n",
    "    'lambda_dist': {'type':'uniform','parameters':[0.05,0.94],'round':True,'decimals':1},\n",
    "    'initial_graph_type': 'barabasi-albert',\n",
    "    'initial_graph_args': {'seed': 0, 'new_node_edges':1},\n",
    "    'device': 'cpu',\n",
    "    'step_count':0,\n",
    "    'step_target':20,\n",
    "    'steering_parameters':{'npath':'./agent_data.zarr',\n",
    "                            'epath':'./edge_data', \n",
    "                            'ndata':['all_except',['a_table']],\n",
    "                            'edata':['all'],\n",
    "                            'mode':'w',\n",
    "                            'wealth_method':'singular_transfer',\n",
    "                            'income_method':'pseudo_income_generation',\n",
    "                            'tech_gamma': torch.tensor([0.3,0.35,0.45]),\n",
    "                            'tech_cost': torch.tensor([0,0.15,0.65]),\n",
    "                            'consume_method':'pseudo_consumption',\n",
    "                            'nn_path': \"/Users/victoria/Documents/Scripts/Python/DGL-PTM/DGL_testing/nn_data/both_PudgeSixLayer_1024/0723_110813/model_best.pth\",\n",
    "                            'adapt_m':torch.tensor([0,0.5,0.9]),\n",
    "                            'adapt_cost':torch.tensor([0,0.25,0.45]),\n",
    "                            'depreciation': 0.6,\n",
    "                            'discount': 0.95,\n",
    "                            'm_theta_dist': {'type':'multinomial','parameters':[[0.02 ,0.03, 0.05, 0.9],[0.7, 0.8, 0.9, 1]],'round':False,'decimals':None},\n",
    "                            'm_attach_dist': {'type':'uniform','parameters':[0.001,1],'round':False,'decimals':None},\n",
    "                            'del_method':'size',\n",
    "                            'del_threshold':0.05,\n",
    "                            'ratio':0.1,\n",
    "                            'weight_a':0.69,\n",
    "                            'weight_b':35, \n",
    "                            'truncation_weight':1.0e-10,\n",
    "                            'step_type':'custom'}})\n",
    "\n",
    "print(model.number_agents)  \n",
    "print(model.steering_parameters)                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using seed 0 for network creation.\n",
      "1 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victoria/Documents/Scripts/Python/DGL-PTM/dgl_ptm/dgl_ptm/model/initialize_model.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dist = torch.gather(torch.Tensor(distParameters[1]), 0, torch.multinomial(torch.tensor(distParameters[0]), nSamples, replacement=True))\n"
     ]
    }
   ],
   "source": [
    "model.initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing step 0 of 20\n",
      "performing step 1 of 20\n",
      "performing step 2 of 20\n",
      "performing step 3 of 20\n",
      "performing step 4 of 20\n",
      "performing step 5 of 20\n",
      "performing step 6 of 20\n",
      "performing step 7 of 20\n",
      "performing step 8 of 20\n",
      "performing step 9 of 20\n",
      "performing step 10 of 20\n",
      "performing step 11 of 20\n",
      "performing step 12 of 20\n",
      "performing step 13 of 20\n",
      "performing step 14 of 20\n",
      "performing step 15 of 20\n",
      "performing step 16 of 20\n",
      "performing step 17 of 20\n",
      "performing step 18 of 20\n",
      "performing step 19 of 20\n"
     ]
    }
   ],
   "source": [
    "model.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "TechTable =  np.array([[0.3,0],[0.35,0.15],[0.45, 0.65]])\n",
    "wealth=torch.tensor([ 6.4072, 17.6514,  8.5695, 27.9308, 13.7166])\n",
    "alpha=torch.tensor([1.0997, 0.9967, 1.0678, 0.9097, 1.0579])\n",
    "gamma = torch.tensor([0.3,0.35,0.45])\n",
    "cost = torch.tensor([0,0.15,0.65])\n",
    "\n",
    "income,index = torch.max((alpha[:,None]*wealth[:,None]**TechTable[:,0] - TechTable[:,1]), axis=1)\n",
    "income=income.to(torch.float32)\n",
    "\n",
    "print(income)\n",
    "print(index)\n",
    "\n",
    "\n",
    "income,tech_index = torch.max((alpha[:,None]*wealth[:,None]**gamma - cost), axis=1)\n",
    "\n",
    "print(income.dtype)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_distribution_tensor(type, distParameters, nSamples, round=False, decimals=None):\n",
    "    \"\"\"\n",
    "    create and return samples from different distributions\n",
    "\n",
    "    :param type: Type of distribution to sample\n",
    "    :param distParameters: array of parameters as required/supported by requested distribution type\n",
    "    :param nSamples: number of samples to return (as 1d tensor)\n",
    "    :param round: optional, whether the samples are to be rounded\n",
    "    :param decimals: optional, required if round is specified. decimal places to round to\n",
    "    \"\"\"\n",
    "    if type == 'uniform':\n",
    "        dist = torch.distributions.uniform.Uniform(torch.tensor(distParameters[0]),torch.tensor(distParameters[1])).sample(torch.tensor([nSamples]))\n",
    "    elif type == 'normal':\n",
    "        dist = torch.distributions.normal.Normal(torch.tensor(distParameters[0]),torch.tensor(distParameters[1])).sample(torch.tensor([nSamples]))\n",
    "    elif type == 'bernoulli':\n",
    "        dist = torch.distributions.bernoulli.Bernoulli(probs=distParameters[0],logits=distParameters[1],validate_args=None).sample(torch.tensor([nSamples]))\n",
    "    elif type == 'multinomial':\n",
    "        dist = torch.gather(torch.Tensor(distParameters[1]), 0, torch.multinomial(torch.tensor(distParameters[0]), nSamples, replacement=True))\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError('Currently only uniform, normal, multinomial, and bernoulli distributions are supported')\n",
    "\n",
    "    if round:\n",
    "        if decimals == None:\n",
    "            raise ValueError('rounding requires decimals of rounding accuracy to be specified')\n",
    "        else:\n",
    "            return torch.round(dist,decimals=decimals)\n",
    "    else:\n",
    "        return dist\n",
    "\n",
    "theta_dist = {'type':'multinomial','parameters':[[0.01 ,0.1, 0.79, 0.1],[0.1, 0.5, 0.8, 1]],'round':False,'decimals':None} \n",
    "theta_vals = torch.tensor([0.1, 0.5, 0.8, 1])\n",
    "capital_dist = {'type':'uniform','parameters':[0.1,10.],'round':False,'decimals':None}\n",
    "\n",
    "\n",
    "agentsCapital = sample_distribution_tensor(capital_dist['type'],capital_dist['parameters'],10,capital_dist['round'],decimals=capital_dist['decimals'])\n",
    "print(agentsCapital)\n",
    "modelTheta = sample_distribution_tensor(theta_dist['type'],theta_dist['parameters'],10,round=theta_dist['round'],decimals=theta_dist['decimals'])\n",
    "print(modelTheta[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_gamma=torch.tensor([0.3,0.35,0.45])\n",
    "tech_cost=torch.tensor([0,0.15,0.65])\n",
    "Œ±=1\n",
    "k=2\n",
    "\n",
    "\n",
    "\n",
    "f = []\n",
    "for i in range(tech_gamma.size(dim=0)): \n",
    "    #in the end, they may need their own tech tables\n",
    "    entry = Œ± * k**tech_gamma[i] - tech_cost[i]\n",
    "    f.append(entry)\n",
    "print(f)\n",
    "entry = Œ± * k**tech_gamma - tech_cost\n",
    "print(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack([tech_gamma,tech_cost]).repeat(4,1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'a':0,'b':4,'c':12}\n",
    "params={'steering_parameters':{'m_theta_dist': {'type':'multinomial','parameters':[[0.01 ,0.1, 0.79, 0.1],[0.1, 0.5, 0.8, 1]],'round':False,'decimals':None},\n",
    "                            'del_prob':0.05},'ndtata':['all']}\n",
    "\n",
    "print(params['steering_parameters']['m_theta_dist']['parameters'][1])\n",
    "\n",
    "\n",
    "def printdata(ndata):\n",
    "    if ndata == ['all']:\n",
    "        ndata = list(data.keys())\n",
    "\n",
    "    if ndata[0] == 'allexcept':\n",
    "        ndata = list(data.keys() - ndata[1])\n",
    "    print(ndata)\n",
    "\n",
    "\n",
    "printdata(['all'])\n",
    "printdata(['allexcept',['a']])\n",
    "printdata(['allexcept',['a','c']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "def income_function(k,Œ±,tech): \n",
    "    f=Œ± * k**tech['gamma'] - tech['cost']\n",
    "    return torch.max(f)\n",
    "\n",
    "\n",
    "class BellmanEquation:\n",
    "    #Adapted from: https://python.quantecon.org/optgrowth.html\n",
    "    def __init__(self,\n",
    "                u,            # utility function\n",
    "                f,            # production function\n",
    "                k,            # current state k_t\n",
    "                Œ∏,            # given shock factor Œ∏\n",
    "                œÉ,            # risk averseness\n",
    "                Œ±,            # human capital\n",
    "                i_a,          # adaptation investment\n",
    "                m,            # protection multiplier\n",
    "                Œ≤,            # discount factor\n",
    "                ùõø,            # depreciation factor \n",
    "                tech):       # adaptation table \n",
    "                #name=\"BellmanNarrowExtended\"\n",
    "                \n",
    "\n",
    "        self.u, self.f, self.k, self.Œ≤, self.Œ∏, self.ùõø, self.œÉ, self.Œ±, self.i_a, self.m, self.tech = u, f, k, Œ≤, Œ∏, ùõø, œÉ, Œ±, i_a, m, tech\n",
    "\n",
    "        # Set up grid\n",
    "        \n",
    "        startgrid=np.array([1.0e-7,1,2,3,4,5,6,7,8,9,10,k+100])\n",
    "\n",
    "        ind=np.searchsorted(startgrid, k)\n",
    "        self.grid=np.concatenate((startgrid[:ind],np.array([k*0.99999, k]),\n",
    "                                startgrid[ind:]))\n",
    "\n",
    "        self.grid=self.grid[self.grid>i_a]\n",
    "\n",
    "        # Identify target state k\n",
    "        self.index = np.searchsorted(self.grid, k)-1\n",
    "    def value(self, c, y, v_array):\n",
    "        \"\"\"\n",
    "        Right hand side of the Bellman equation.\n",
    "        \"\"\"\n",
    "\n",
    "        u, f, Œ≤, Œ∏, ùõø, œÉ, Œ±, i_a, m, tech = self.u, self.f, self.Œ≤, self.Œ∏, self.ùõø, self.œÉ, self.Œ±, self.i_a, self.m, self.tech\n",
    "\n",
    "        v = interp1d(self.grid, v_array, bounds_error=False, \n",
    "                    fill_value=\"extrapolate\")\n",
    "        return u(c,œÉ) + Œ≤ * v((Œ∏ + m * (1-Œ∏)) * (f(y,Œ±,tech) - c - i_a + (1 - ùõø) * y))\n",
    "\n",
    "\n",
    "def _optimized_wealth_consumption(model_graph, model_params):\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "    def maximize(g, a, b, args):\n",
    "        \"\"\"\n",
    "        From: https://python.quantecon.org/optgrowth.html (similar example \n",
    "        https://macroeconomics.github.io/Dynamic%20Programming.html#.ZC13-exBy3I)\n",
    "        Maximize the function g over the interval [a, b].\n",
    "\n",
    "        The maximizer of g on any interval is\n",
    "        also the minimizer of -g.  The tuple args collects any extra\n",
    "        arguments to g.\n",
    "\n",
    "        Returns the maximum value and the maximizer.\n",
    "        \"\"\"\n",
    "\n",
    "        objective = lambda x: -g(x, *args)\n",
    "        result = minimize_scalar(objective, bounds=(a, b), method='bounded')\n",
    "        maximizer, maximum = result.x, -result.fun\n",
    "        return maximizer, maximum\n",
    "\n",
    "    def utility(c, œÉ, type=\"isoelastic\"):\n",
    "        if type == \"isoelastic\":\n",
    "            if œÉ ==1:\n",
    "                return np.log(c)\n",
    "            else:\n",
    "                return (c**(1-œÉ)-1)/(1-œÉ)\n",
    "\n",
    "        else:\n",
    "            print(\"Unspecified utility function!!!\")\n",
    "\n",
    "\n",
    "    def update_bellman(v, bell):\n",
    "        \"\"\"\n",
    "        From: https://python.quantecon.org/optgrowth.html (similar example\n",
    "        https://macroeconomics.github.io/Dynamic%20Programming.html#.ZC13-exBy3I)\n",
    "        \n",
    "        The Bellman operator.  Updates the guess of the value function\n",
    "        and also computes a v-greedy policy.\n",
    "\n",
    "        * bell is an instance of Bellman equation\n",
    "        * v is an array representing a guess of the value function\n",
    "\n",
    "        \"\"\"\n",
    "        v_new = np.empty_like(v)\n",
    "        v_greedy = np.empty_like(v)\n",
    "        \n",
    "        for i in range(len(bell.grid)):\n",
    "            y = bell.grid[i]\n",
    "            # Maximize RHS of Bellman equation at state y\n",
    "            \n",
    "            c_star, v_max = maximize(bell.value, min([1e-8,y*0.00001]), \n",
    "                                    y-bell.i_a, (y, v))\n",
    "            #VMG HELP! can anyone check that (1) subtracting i_a and \n",
    "            # (2) omitting any grid values less than i_a \n",
    "            # will not be problematic? The only thing I can come up with\n",
    "            # is if i_a is greater than k*0.99999\n",
    "            # which_bellman() now accounts for that case. Whole thing \n",
    "            # could use refinement.\n",
    "        \n",
    "            v_new[i] = v_max\n",
    "            v_greedy[i] = c_star\n",
    "\n",
    "        return v_greedy, v_new\n",
    "\n",
    "\n",
    "    def which_bellman(agentinfo):\n",
    "        \"\"\"\n",
    "        Solves bellman for each affordable adaptation option.\n",
    "        \"\"\"\n",
    "        feasible=[]\n",
    "\n",
    "        for option in torch.transpose(agentinfo['adapt'],0,1):\n",
    "            if option[1]>=((income_function(agentinfo['k'],agentinfo['Œ±'],tech={'gamma':model_params['tech_gamma'],'cost':model_params['tech_cost']})+(1 - model_params['depreciation'])*agentinfo['k'])*.99998):\n",
    "                # ensures that the gridpoint\n",
    "                # just below k, k*0.99999, is included\n",
    "                pass\n",
    "            else:\n",
    "                #  print(f'working theta = {agentinfo.Œ∏ + option[0] *\\\n",
    "                #  (1-agentinfo.Œ∏)}, i_a= {option[1]}, k= {agentinfo.k}')\n",
    "                c,v=solve_bellman(BellmanEquation(u=utility, \n",
    "                                f=income_function, k=agentinfo['k'], \n",
    "                                Œ∏=agentinfo['Œ∏'], œÉ=agentinfo['œÉ'], \n",
    "                                Œ±=agentinfo['Œ±'], i_a=option[1].numpy(),m=option[0],\n",
    "                                Œ≤=model_params['discount'], ùõø=model_params['depreciation'],\n",
    "                                tech={'gamma':model_params['tech_gamma'],'cost':model_params['tech_cost']}))\n",
    "                feasible.append([v,c,option[1],option[0]])\n",
    "\n",
    "        best=min(feasible)\n",
    "\n",
    "        return best[1],best[2],best[3]\n",
    "\n",
    "    def solve_bellman(bell,\n",
    "                    tol=1,\n",
    "                    min_iter=10,\n",
    "                    max_iter=1000,\n",
    "                    verbose=False):\n",
    "        \"\"\"\n",
    "        From: https://python.quantecon.org/optgrowth.html (similar example\n",
    "        https://macroeconomics.github.io/Dynamic%20Programming.html#.ZC13-exBy3I)\n",
    "        \n",
    "        Solve model by iterating with the Bellman operator.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # Set up loop\n",
    "\n",
    "        v = bell.u(bell.grid,bell.œÉ)  # Initial condition\n",
    "        i = 0\n",
    "        error = tol + 1\n",
    "\n",
    "        while (i < max_iter and error > tol) or (i < min_iter):\n",
    "            v_greedy, v_new = update_bellman(v, bell)\n",
    "            error = np.abs(v[bell.index] - v_new)[bell.index]\n",
    "            i += 1\n",
    "            # if verbose and i % print_skip == 0:\n",
    "            #     print(f\"Error at iteration {i} is {error}.\")\n",
    "            v = v_new\n",
    "\n",
    "        if error > tol:\n",
    "            print(f\"{bell.name} failed to converge for k={bell.k}, Œ± = {bell.Œ±},œÉ ={bell.œÉ}, i_a={bell.i_a}, and modified Œ∏ = {bell.Œ∏ + bell.m * (1-bell.Œ∏)}!\")\n",
    "        elif verbose:\n",
    "            print(f\"Converged in {i} iterations.\")\n",
    "            print(f\"Effective k and new c {np.around(bell.grid[bell.index],3),v_greedy[bell.index]}\")\n",
    "            \n",
    "\n",
    "        return v_greedy[bell.index],v[bell.index]\n",
    "    \n",
    "    model_graph['wealth_consumption'], model_graph['i_a'], model_graph['m'] = torch.zeros(9),torch.zeros(9),torch.zeros(9)\n",
    "    for i in range(model_params['number_agents']):\n",
    "        agentinfo = {'u':utility, 'f':income_function, 'Œ≤':model_params['discount'], 'Œ∏':model_graph['theta'][i], 'ùõø':model_params['depreciation'], 'œÉ':model_graph['sigma'][i].numpy(), 'Œ±': model_graph['alpha'][i],'k':model_graph['wealth'][i],'adapt': model_graph['a_table'][i]}\n",
    "        model_graph['wealth_consumption'][i], model_graph['i_a'][i], model_graph['m'][i] = which_bellman(agentinfo)\n",
    "    print(model_graph)\n",
    "\n",
    "modelpar={'discount':0.95,'depreciation':0.08,'number_agents':9,'tech_gamma': torch.tensor([0.3,0.35,0.45]),'tech_cost': torch.tensor([0,0.15,0.65]),'num_nodes':9}        \n",
    "ndata={'theta':torch.tensor([0.7603, 0.7641, 0.9275, 0.7906, 0.7858, 0.7832, 0.3904, 0.3924, 0.7884]),'sigma':torch.tensor([1.3000, 1.8000, 0.6000, 0.7000, 1.2000, 1.1000, 0.7000, 1.8000, 1.4000]),'alpha':torch.tensor([0.9952, 1.1045, 1.0706, 1.0489, 1.0576, 1.1922, 1.0162, 1.0938, 1.1324]),'wealth':torch.tensor([0.9998, 1.1309, 1.1439, 1.0342, 1.2321, 1.0819, 1.0326, 1.1193, 1.0707]), 'a_table':torch.stack([torch.tensor([0,0.5,0.9]),torch.tensor([0,0.25,0.45])]).repeat(modelpar['number_agents'],1,1)}\n",
    "_optimized_wealth_consumption(ndata,modelpar)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class torchInterp1d:\n",
    "    \"\"\"\n",
    "    PyTorch-friendly, 1-D linear interpolation.\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "       \n",
    "\n",
    "    def __call__(self, x_target):\n",
    "        x = self.x\n",
    "        y = self.y\n",
    "\n",
    "        y_target = torch.empty_like(x_target)\n",
    "\n",
    "        for i, x_val in enumerate(x_target):\n",
    "            if x_val < x[0]:\n",
    "                x_1,y_1,x_2,y_2 = x[0],y[0],x[1],y[1]\n",
    "                y_base,x_base = y[0], x[0] \n",
    "            elif x_val > x[-1]:\n",
    "                x_1,y_1,x_2,y_2=x[-2],y[-2],x[-1],y[-1]\n",
    "                y_base,x_base = y[-1],x[-1]\n",
    "            else:\n",
    "                ind = torch.searchsorted(x, x_val)\n",
    "                if ind==14:\n",
    "                    print(x)\n",
    "                    print(y)\n",
    "                    print(x_val)\n",
    "                x_1,y_1,x_2,y_2 = x[ind-1],y[ind-1],x[ind],y[ind]\n",
    "                y_base,x_base=y[ind-1],x[ind-1]\n",
    "\n",
    "\n",
    "            y_target[i] = y_base + (x_val - x_base) * (y_2-y_1)/(x_2-x_1)\n",
    "\n",
    "        return y_target\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "startgrid=torch.tensor([1.0e-7,1,2,3,4,5,6,7,8,9,10,k+100])\n",
    "\n",
    "ind=torch.searchsorted(startgrid, k)\n",
    "grid=torch.cat((startgrid[:ind],torch.tensor([k*0.99999, k]),\n",
    "                        startgrid[ind:]))\n",
    "\n",
    "\n",
    "\n",
    "v_array =grid/3\n",
    "print(type(v_array))\n",
    "print(type(grid))\n",
    "v = interp1d(grid, v_array, bounds_error=False, \n",
    "            fill_value=\"extrapolate\")\n",
    "print(v(1.1),v(1600))\n",
    "\n",
    "\n",
    "v = torchInterp1d(grid, v_array)\n",
    "print(v(torch.tensor([1.1,1600])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def income_function(k,Œ±,tech): \n",
    "    f=Œ± * k.unsqueeze(-1)**tech['gamma'] - tech['cost']\n",
    "    return torch.max(f,-1)[0]\n",
    "\n",
    "class TensBellmanEquation:\n",
    "    #Adapted from: https://python.quantecon.org/optgrowth.html\n",
    "    def __init__(self,\n",
    "                u,            # utility function\n",
    "                f,            # production function\n",
    "                k,            # current state k_t\n",
    "                Œ∏,            # given shock factor Œ∏\n",
    "                œÉ,            # risk averseness\n",
    "                Œ±,            # human capital\n",
    "                i_a,          # adaptation investment\n",
    "                m,            # protection multiplier\n",
    "                Œ≤,            # discount factor\n",
    "                ùõø,            # depreciation factor \n",
    "                tech):       # adaptation table \n",
    "                #name=\"BellmanNarrowExtended\"\n",
    "                \n",
    "\n",
    "        self.u, self.f, self.k, self.Œ≤, self.Œ∏, self.ùõø, self.œÉ, self.Œ±, self.i_a, self.m, self.tech = u, f, k, Œ≤, Œ∏, ùõø, œÉ, Œ±, i_a, m, tech\n",
    "\n",
    "        # Set up grid\n",
    "        \n",
    "        startgrid=torch.tensor([1.0e-7,1,2,3,4,5,6,7,8,9,10,k+100])\n",
    "\n",
    "        ind=torch.searchsorted(startgrid, k)\n",
    "        self.grid=torch.cat((startgrid[:ind],torch.tensor([k*0.99999, k]),\n",
    "                                startgrid[ind:]))\n",
    "\n",
    "        self.grid=self.grid[self.grid>i_a]\n",
    "        # Identify target state k\n",
    "        self.index = torch.searchsorted(self.grid, k)-1\n",
    "    def value(self, c, y, v_array):\n",
    "        \"\"\"\n",
    "        Right hand side of the Bellman equation.\n",
    "        \"\"\"\n",
    "\n",
    "        u, f, Œ≤, Œ∏, ùõø, œÉ, Œ±, i_a, m, tech = self.u, self.f, self.Œ≤, self.Œ∏, self.ùõø, self.œÉ, self.Œ±, self.i_a, self.m, self.tech\n",
    "\n",
    "        v = torchInterp1d(self.grid, v_array)\n",
    "\n",
    "        return u(c,œÉ) + Œ≤ * v((Œ∏ + m * (1-Œ∏)) * (f(y,Œ±,tech) - c - i_a + (1 - ùõø) * y))\n",
    "\n",
    "\n",
    "\n",
    "def _torch_optimized_wealth_consumption(model_graph, model_params):\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def torchmaximize(g, a, b, args):\n",
    "        \"\"\"\n",
    "        Maximize the function g over the interval [a, b] using PyTorch's LBFGS optimizer.\n",
    "\n",
    "        The tuple args collects any extra arguments to g.\n",
    "\n",
    "        Returns the maximal value and the maximizer.\n",
    "        \"\"\"\n",
    "\n",
    "        # Define a closure function  to reset gradient, determine value, and calculate new gradient\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            value = -g(x, *args)\n",
    "            loss= torch.sum(value)\n",
    "            loss.backward(retain_graph=True)\n",
    "            return loss\n",
    "\n",
    "        # Initial guess for consumption\n",
    "        x = (args[0]/3).clone().requires_grad_(True)\n",
    "\n",
    "        # Feed it to optimizer and keep x guesses in the bounds\n",
    "        optimizer = torch.optim.LBFGS([x], history_size=10, max_iter=4)\n",
    "        x.data = torch.clamp(x, a, b)\n",
    "\n",
    "        #history=[]\n",
    "        # Run optimization for 30 epochs:\n",
    "        for epoch in range(30):\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for i, c in enumerate(x):  # You can adjust the number of optimization steps\n",
    "                #history.append(g(x,*args))\n",
    "                optimizer.step(closure)\n",
    "                loss = closure()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch: {epoch + 1:02}/{30} Loss: {running_loss:.5e}\")\n",
    "\n",
    "\n",
    "\n",
    "        maximizer = x\n",
    "        maximum = -g(x, *args)\n",
    "        print(maximizer)\n",
    "        print(maximum)\n",
    "        #print(history)\n",
    "        return maximizer, maximum\n",
    "\n",
    "    def utility(c, œÉ, type=\"isoelastic\"):\n",
    "        if type == \"isoelastic\":\n",
    "            if œÉ ==1:\n",
    "                return torch.log(c)\n",
    "            else:\n",
    "                return (c**(1-œÉ)-1)/(1-œÉ)\n",
    "\n",
    "        else:\n",
    "            print(\"Unspecified utility function!!!\")\n",
    "\n",
    "\n",
    "    def update_bellman(v, bell):\n",
    "        \"\"\"\n",
    "        From: https://python.quantecon.org/optgrowth.html (similar example\n",
    "        https://macroeconomics.github.io/Dynamic%20Programming.html#.ZC13-exBy3I)\n",
    "        \n",
    "        The Bellman operator.  Updates the guess of the value function\n",
    "        and also computes a v-greedy policy.\n",
    "\n",
    "        * bell is an instance of Bellman equation\n",
    "        * v is an array representing a guess of the value function\n",
    "\n",
    "        \"\"\"\n",
    "        v_new = torch.empty_like(v)\n",
    "        v_greedy = torch.empty_like(v)\n",
    "        \n",
    "        print('checkpoint2')\n",
    "\n",
    "        y = bell.grid\n",
    "        # Maximize RHS of Bellman equation at state y\n",
    "        a=torch.min(torch.transpose(torch.stack((torch.full(y.size(),1e-8),(y*0.00001)),0),0,1),1)[0]\n",
    "        b=(y-bell.i_a)\n",
    "\n",
    "        #pairs = torch.transpose(torch.stack((a,b),0),1,0).tolist()\n",
    "        #boundsequence = [tuple(x) for x in pairs]\n",
    "\n",
    "        c_star, v_max = torchmaximize(bell.value, a, b, (y, v))\n",
    "            #VMG HELP! can anyone check that (1) subtracting i_a and \n",
    "            # (2) omitting any grid values less than i_a \n",
    "            # will not be problematic? The only thing I can come up with\n",
    "            # is if i_a is greater than k*0.99999\n",
    "            # which_bellman() now accounts for that case. Whole thing \n",
    "            # could use refinement.\n",
    "\n",
    "        v_new = v_max\n",
    "        v_greedy = c_star\n",
    "\n",
    "        return v_greedy, v_new\n",
    "\n",
    "\n",
    "    def which_bellman(agentinfo):\n",
    "        \"\"\"\n",
    "        Solves bellman for each affordable adaptation option.\n",
    "        \"\"\"\n",
    "        feasible=[]\n",
    "\n",
    "        for option in torch.transpose(agentinfo['adapt'],0,1):\n",
    "            if option[1]>=(income_function(agentinfo['k'],agentinfo['Œ±'],tech={'gamma':model_params['tech_gamma'],'cost':model_params['tech_cost']})+(1 - model_params['depreciation'])*agentinfo['k'])*.99998:\n",
    "                # ensures that the gridpoint\n",
    "                # just below k, k*0.99999, is included\n",
    "                pass\n",
    "            else:\n",
    "                #  print(f'working theta = {agentinfo.Œ∏ + option[0] *\\\n",
    "                #  (1-agentinfo.Œ∏)}, i_a= {option[1]}, k= {agentinfo.k}')\n",
    "                c,v=solve_bellman(TensBellmanEquation(u=utility, \n",
    "                                f=income_function, k=agentinfo['k'], \n",
    "                                Œ∏=agentinfo['Œ∏'], œÉ=agentinfo['œÉ'], \n",
    "                                Œ±=agentinfo['Œ±'], i_a=option[1],m=option[0],\n",
    "                                Œ≤=model_params['discount'], ùõø=model_params['depreciation'],\n",
    "                                tech={'gamma':model_params['tech_gamma'],'cost':model_params['tech_cost']}))\n",
    "                feasible.append([v,c,option[1],option[0]])\n",
    "\n",
    "        best=min(feasible)\n",
    "\n",
    "        return best[1],best[2],best[3]\n",
    "\n",
    "    def solve_bellman(bell,\n",
    "                    tol=1,\n",
    "                    min_iter=10,\n",
    "                    max_iter=1000,\n",
    "                    verbose=False):\n",
    "        \"\"\"\n",
    "        From: https://python.quantecon.org/optgrowth.html (similar example\n",
    "        https://macroeconomics.github.io/Dynamic%20Programming.html#.ZC13-exBy3I)\n",
    "        \n",
    "        Solve model by iterating with the Bellman operator.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Set up loop\n",
    "\n",
    "        v = bell.u(bell.grid,bell.œÉ)  # Initial condition\n",
    "        i = 0\n",
    "        error = tol + 1\n",
    "        while (i < max_iter and error > tol) or (i < min_iter):\n",
    "            print('checkpoint1')\n",
    "            v_greedy, v_new = update_bellman(v, bell)\n",
    "            print('checkpoint3')\n",
    "            error = torch.abs(v[bell.index] - v_new)[bell.index]\n",
    "            print(error)\n",
    "            i += 1\n",
    "            # if verbose and i % print_skip == 0:\n",
    "            #     print(f\"Error at iteration {i} is {error}.\")\n",
    "            v = v_new\n",
    "\n",
    "        print('checkpoint4')\n",
    "\n",
    "        if error > tol:\n",
    "            print(f\"{bell.name} failed to converge for k={bell.k}, Œ± = {bell.Œ±},œÉ ={bell.œÉ}, i_a={bell.i_a}, and modified Œ∏ = {bell.Œ∏ + bell.m * (1-bell.Œ∏)}!\")\n",
    "        elif verbose:\n",
    "            print(f\"Converged in {i} iterations.\")\n",
    "            print(f\"Effective k and new c {torch.round(bell.grid[bell.index],3),v_greedy[bell.index]}\")\n",
    "            \n",
    "\n",
    "        return v_greedy[bell.index],v[bell.index]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    model_graph['wealth_consumption'], model_graph['i_a'], model_graph['m'] = torch.zeros(9),torch.zeros(9),torch.zeros(9)\n",
    "    for i in range(model_params['number_agents']):\n",
    "        agentinfo = {'u':utility, 'f':income_function, 'Œ≤':model_params['discount'], 'Œ∏':model_graph['theta'][i], 'ùõø':model_params['depreciation'], 'œÉ':model_graph['sigma'][i].numpy(), 'Œ±': model_graph['alpha'][i],'k':model_graph['wealth'][i],'adapt': model_graph['a_table'][i]}\n",
    "        model_graph['wealth_consumption'][i], model_graph['i_a'][i], model_graph['m'][i] = which_bellman(agentinfo)\n",
    "    print(model_graph)\n",
    "\n",
    "modelpar={'discount':0.95,'depreciation':0.08,'number_agents':9,'tech_gamma': torch.tensor([0.3,0.35,0.45]),'tech_cost': torch.tensor([0,0.15,0.65]),'num_nodes':9}        \n",
    "ndata={'theta':torch.tensor([0.7603, 0.7641, 0.9275, 0.7906, 0.7858, 0.7832, 0.3904, 0.3924, 0.7884]),'sigma':torch.tensor([1.3000, 1.8000, 0.6000, 0.7000, 1.2000, 1.1000, 0.7000, 1.8000, 1.4000]),'alpha':torch.tensor([0.9952, 1.1045, 1.0706, 1.0489, 1.0576, 1.1922, 1.0162, 1.0938, 1.1324]),'wealth':torch.tensor([0.9998, 1.1309, 1.1439, 1.0342, 1.2321, 1.0819, 1.0326, 1.1193, 1.0707]), 'a_table':torch.stack([torch.tensor([0,0.5,0.9]),torch.tensor([0,0.25,0.45])]).repeat(modelpar['number_agents'],1,1)}\n",
    "_torch_optimized_wealth_consumption(ndata,modelpar)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base=torch.tensor([1.0000e-07, 1.0100e+02])\n",
    "coef=torch.tensor(0.9952)\n",
    "powers=torch.tensor([0.3000, 0.3500, 0.4500])\n",
    "cost=torch.tensor([0,0.15,0.65])\n",
    "#powers=torch.transpose(powers,-1,0)#.repeat(4,1)\n",
    "print(powers.dtype)\n",
    "print(base.dtype)\n",
    "print(coef.dtype)\n",
    "r1=coef * base[:, None] ** powers - cost\n",
    "torch.max(r1,-1)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def income_function(k,Œ±,tech): \n",
    "    f=Œ± * k.unsqueeze(-1)**tech['gamma'] - tech['cost']\n",
    "    return torch.max(f,-1)[0]\n",
    "\n",
    "class TensBellmanEquation:\n",
    "    #Adapted from: https://python.quantecon.org/optgrowth.html\n",
    "    def __init__(self,\n",
    "                u,            # utility function\n",
    "                f,            # production function\n",
    "                k,            # current state k_t\n",
    "                Œ∏,            # given shock factor Œ∏\n",
    "                œÉ,            # risk averseness\n",
    "                Œ±,            # human capital\n",
    "                i_a,          # adaptation investment\n",
    "                m,            # protection multiplier\n",
    "                Œ≤,            # discount factor\n",
    "                ùõø,            # depreciation factor \n",
    "                tech):       # adaptation table \n",
    "                #name=\"BellmanNarrowExtended\"\n",
    "                \n",
    "\n",
    "        self.u, self.f, self.k, self.Œ≤, self.Œ∏, self.ùõø, self.œÉ, self.Œ±, self.i_a, self.m, self.tech = u, f, k, Œ≤, Œ∏, ùõø, œÉ, Œ±, i_a, m, tech\n",
    "\n",
    "        # Set up grid\n",
    "        \n",
    "        startgrid=torch.tensor([1.0e-7,1,2,3,4,5,6,7,8,9,10,k+100])\n",
    "\n",
    "        ind=torch.searchsorted(startgrid, k)\n",
    "        self.grid=torch.cat((startgrid[:ind],torch.tensor([k*0.99999, k]),\n",
    "                                startgrid[ind:]))\n",
    "\n",
    "        self.grid=self.grid[self.grid>i_a]\n",
    "        # Identify target state k\n",
    "        self.index = torch.searchsorted(self.grid, k)-1\n",
    "    def value(self, c, y, v_array):\n",
    "        \"\"\"\n",
    "        Right hand side of the Bellman equation.\n",
    "        \"\"\"\n",
    "\n",
    "        u, f, Œ≤, Œ∏, ùõø, œÉ, Œ±, i_a, m, tech = self.u, self.f, self.Œ≤, self.Œ∏, self.ùõø, self.œÉ, self.Œ±, self.i_a, self.m, self.tech\n",
    "\n",
    "        v = torchInterp1d(self.grid, v_array)\n",
    "\n",
    "        return u(c,œÉ) + Œ≤ * v((Œ∏ + m * (1-Œ∏)) * (f(y,Œ±,tech) - c - i_a + (1 - ùõø) * y))\n",
    "\n",
    "\n",
    "\n",
    "def _torch_optimized_wealth_consumption(model_graph, model_params):\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def torchmaximize(g, a, b, args):\n",
    "        \"\"\"\n",
    "        Maximize the function g over the interval [a, b] using PyTorch's LBFGS optimizer.\n",
    "\n",
    "        The tuple args collects any extra arguments to g.\n",
    "\n",
    "        Returns the maximal value and the maximizer.\n",
    "        \"\"\"\n",
    "\n",
    "        # Define a closure function  to reset gradient, determine value, and calculate new gradient\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            value = -g(x, *args)\n",
    "            loss= torch.sum(value)\n",
    "            loss.backward(retain_graph=True)\n",
    "            return loss\n",
    "\n",
    "        # Initial guess for consumption\n",
    "        x = (args[0]/3).clone().requires_grad_(True)\n",
    "\n",
    "        # Feed it to optimizer and keep x guesses in the bounds\n",
    "        optimizer = torch.optim.LBFGS([x], history_size=10, max_iter=4)\n",
    "        x.data = torch.clamp(x, a, b)\n",
    "\n",
    "        #history=[]\n",
    "        # Run optimization for 30 epochs:\n",
    "        for epoch in range(30):\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for i, c in enumerate(x):  # You can adjust the number of optimization steps\n",
    "                #history.append(g(x,*args))\n",
    "                optimizer.step(closure)\n",
    "                loss = closure()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch: {epoch + 1:02}/{30} Loss: {running_loss:.5e}\")\n",
    "\n",
    "\n",
    "\n",
    "        maximizer = x\n",
    "        maximum = -g(x, *args)\n",
    "        print(maximizer)\n",
    "        print(maximum)\n",
    "        #print(history)\n",
    "        return maximizer, maximum\n",
    "\n",
    "    def utility(c, œÉ, type=\"isoelastic\"):\n",
    "        if type == \"isoelastic\":\n",
    "            if œÉ ==1:\n",
    "                return torch.log(c)\n",
    "            else:\n",
    "                return (c**(1-œÉ)-1)/(1-œÉ)\n",
    "\n",
    "        else:\n",
    "            print(\"Unspecified utility function!!!\")\n",
    "\n",
    "\n",
    "    def update_bellman(v, bell):\n",
    "        \"\"\"\n",
    "        From: https://python.quantecon.org/optgrowth.html (similar example\n",
    "        https://macroeconomics.github.io/Dynamic%20Programming.html#.ZC13-exBy3I)\n",
    "        \n",
    "        The Bellman operator.  Updates the guess of the value function\n",
    "        and also computes a v-greedy policy.\n",
    "\n",
    "        * bell is an instance of Bellman equation\n",
    "        * v is an array representing a guess of the value function\n",
    "\n",
    "        \"\"\"\n",
    "        v_new = torch.empty_like(v)\n",
    "        v_greedy = torch.empty_like(v)\n",
    "        \n",
    "        print('checkpoint2')\n",
    "\n",
    "        y = bell.grid\n",
    "        # Maximize RHS of Bellman equation at state y\n",
    "        a=torch.min(torch.transpose(torch.stack((torch.full(y.size(),1e-8),(y*0.00001)),0),0,1),1)[0]\n",
    "        b=(y-bell.i_a)\n",
    "\n",
    "        #pairs = torch.transpose(torch.stack((a,b),0),1,0).tolist()\n",
    "        #boundsequence = [tuple(x) for x in pairs]\n",
    "\n",
    "        c_star, v_max = torchmaximize(bell.value, a, b, (y, v))\n",
    "            #VMG HELP! can anyone check that (1) subtracting i_a and \n",
    "            # (2) omitting any grid values less than i_a \n",
    "            # will not be problematic? The only thing I can come up with\n",
    "            # is if i_a is greater than k*0.99999\n",
    "            # which_bellman() now accounts for that case. Whole thing \n",
    "            # could use refinement.\n",
    "\n",
    "        v_new = v_max\n",
    "        v_greedy = c_star\n",
    "\n",
    "        return v_greedy, v_new\n",
    "\n",
    "\n",
    "    def which_bellman(agentinfo):\n",
    "        \"\"\"\n",
    "        Solves bellman for each affordable adaptation option.\n",
    "        \"\"\"\n",
    "        feasible=[]\n",
    "\n",
    "        for option in torch.transpose(agentinfo['adapt'],0,1):\n",
    "            if option[1]>=(income_function(agentinfo['k'],agentinfo['Œ±'],tech={'gamma':model_params['tech_gamma'],'cost':model_params['tech_cost']})+(1 - model_params['depreciation'])*agentinfo['k'])*.99998:\n",
    "                # ensures that the gridpoint\n",
    "                # just below k, k*0.99999, is included\n",
    "                pass\n",
    "            else:\n",
    "                #  print(f'working theta = {agentinfo.Œ∏ + option[0] *\\\n",
    "                #  (1-agentinfo.Œ∏)}, i_a= {option[1]}, k= {agentinfo.k}')\n",
    "                c,v=solve_bellman(TensBellmanEquation(u=utility, \n",
    "                                f=income_function, k=agentinfo['k'], \n",
    "                                Œ∏=agentinfo['Œ∏'], œÉ=agentinfo['œÉ'], \n",
    "                                Œ±=agentinfo['Œ±'], i_a=option[1],m=option[0],\n",
    "                                Œ≤=model_params['discount'], ùõø=model_params['depreciation'],\n",
    "                                tech={'gamma':model_params['tech_gamma'],'cost':model_params['tech_cost']}))\n",
    "                feasible.append([v,c,option[1],option[0]])\n",
    "\n",
    "        best=min(feasible)\n",
    "\n",
    "        return best[1],best[2],best[3]\n",
    "\n",
    "    def solve_bellman(bell,\n",
    "                    tol=1,\n",
    "                    min_iter=10,\n",
    "                    max_iter=1000,\n",
    "                    verbose=False):\n",
    "        \"\"\"\n",
    "        From: https://python.quantecon.org/optgrowth.html (similar example\n",
    "        https://macroeconomics.github.io/Dynamic%20Programming.html#.ZC13-exBy3I)\n",
    "        \n",
    "        Solve model by iterating with the Bellman operator.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Set up loop\n",
    "\n",
    "        v = bell.u(bell.grid,bell.œÉ)  # Initial condition\n",
    "        i = 0\n",
    "        error = tol + 1\n",
    "        while (i < max_iter and error > tol) or (i < min_iter):\n",
    "            print('checkpoint1')\n",
    "            v_greedy, v_new = update_bellman(v, bell)\n",
    "            print('checkpoint3')\n",
    "            error = torch.abs(v[bell.index] - v_new)[bell.index]\n",
    "            print(error)\n",
    "            i += 1\n",
    "            # if verbose and i % print_skip == 0:\n",
    "            #     print(f\"Error at iteration {i} is {error}.\")\n",
    "            v = v_new\n",
    "\n",
    "        print('checkpoint4')\n",
    "\n",
    "        if error > tol:\n",
    "            print(f\"{bell.name} failed to converge for k={bell.k}, Œ± = {bell.Œ±},œÉ ={bell.œÉ}, i_a={bell.i_a}, and modified Œ∏ = {bell.Œ∏ + bell.m * (1-bell.Œ∏)}!\")\n",
    "        elif verbose:\n",
    "            print(f\"Converged in {i} iterations.\")\n",
    "            print(f\"Effective k and new c {torch.round(bell.grid[bell.index],3),v_greedy[bell.index]}\")\n",
    "            \n",
    "\n",
    "        return v_greedy[bell.index],v[bell.index]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    model_graph['wealth_consumption'], model_graph['i_a'], model_graph['m'] = torch.zeros(9),torch.zeros(9),torch.zeros(9)\n",
    "    for i in range(model_params['number_agents']):\n",
    "        agentinfo = {'u':utility, 'f':income_function, 'Œ≤':model_params['discount'], 'Œ∏':model_graph['theta'][i], 'ùõø':model_params['depreciation'], 'œÉ':model_graph['sigma'][i].numpy(), 'Œ±': model_graph['alpha'][i],'k':model_graph['wealth'][i],'adapt': model_graph['a_table'][i]}\n",
    "        model_graph['wealth_consumption'][i], model_graph['i_a'][i], model_graph['m'][i] = which_bellman(agentinfo)\n",
    "    print(model_graph)\n",
    "\n",
    "modelpar={'discount':0.95,'depreciation':0.08,'number_agents':9,'tech_gamma': torch.tensor([0.3,0.35,0.45]),'tech_cost': torch.tensor([0,0.15,0.65]),'num_nodes':9}        \n",
    "ndata={'theta':torch.tensor([0.7603, 0.7641, 0.9275, 0.7906, 0.7858, 0.7832, 0.3904, 0.3924, 0.7884]),'sigma':torch.tensor([1.3000, 1.8000, 0.6000, 0.7000, 1.2000, 1.1000, 0.7000, 1.8000, 1.4000]),'alpha':torch.tensor([0.9952, 1.1045, 1.0706, 1.0489, 1.0576, 1.1922, 1.0162, 1.0938, 1.1324]),'wealth':torch.tensor([0.9998, 1.1309, 1.1439, 1.0342, 1.2321, 1.0819, 1.0326, 1.1193, 1.0707]), 'a_table':torch.stack([torch.tensor([0,0.5,0.9]),torch.tensor([0,0.25,0.45])]).repeat(modelpar['number_agents'],1,1)}\n",
    "_torch_optimized_wealth_consumption(ndata,modelpar)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('dgl_ptm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1477468427ea6fe7f3c6460347a373a01cc68daae53faf91f7d9fb578ee805b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
